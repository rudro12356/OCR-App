{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-18T14:58:02.476372Z",
     "start_time": "2025-03-18T14:58:02.470894Z"
    }
   },
   "source": [
    "import fitz\n",
    "import pymupdf"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T14:59:44.220165Z",
     "start_time": "2025-03-18T14:59:44.061724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "doc = pymupdf.open(\"/Users/home/OCR APP/OCR-App/Data/2308.08859v2.pdf\")\n",
    "out = open(\"output.txt\", \"wb\")\n",
    "\n",
    "for page in doc:\n",
    "    text = page.get_text().encode(\"utf-8\")\n",
    "    out.write(text)\n",
    "    out.write(bytes((12,)))\n",
    "out.close()"
   ],
   "id": "15e90ca9b30df655",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T15:18:59.398685Z",
     "start_time": "2025-03-18T15:17:52.679148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unstructured.partition.auto import partition\n",
    "\n",
    "elements = partition(\"/Users/home/OCR APP/OCR-App/Data/2308.08859v2.pdf\")\n",
    "print(\"\\n\\n\".join([str(el) for el in elements]))"
   ],
   "id": "8e5ffc2479eea95f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/OCR_APP/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2 0 2\n",
      "\n",
      "t c O 2 1\n",
      "\n",
      "] h p - m e h c . s c i s y h p [\n",
      "\n",
      "2 v 9 5 8 8 0 . 8 0 3 2 : v i X r a\n",
      "\n",
      "Tutorial: How to Train a Neural Network Potential\n",
      "\n",
      "Alea Miako Tokita∗ and J¨org Behler† Lehrstuhl f¨ur Theoretische Chemie II, Ruhr-Universit¨at Bochum, 44780 Bochum, Germany and Research Center Chemical Sciences and Sustainability, Research Alliance Ruhr, 44780 Bochum, Germany\n",
      "\n",
      "The introduction of modern Machine Learning Potentials (MLP) has led to a paradigm change in the development of potential energy surfaces for atomistic simulations. By providing efficient access to energies and forces, they allow us to perform large-scale simulations of extended systems, which are not directly accessible by demanding first-principles methods. In these simulations, MLPs can reach the accuracy of electronic structure calculations provided that they have been properly trained and validated using a suitable set of reference data. Due to their highly flexible functional form the construction of MLPs has to be done with great care. In this Tutorial, we describe the necessary key steps for training reliable MLPs, from data generation via training to final validation. The procedure, which is illustrated for the example of a high-dimensional neural network potential, is general and applicable to many types of MLPs.\n",
      "\n",
      "I.\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "In recent decades, advances in atomistic simulations have revolutionized the way of studying complex systems in many fields, from chemistry and physics via mate- rials science to the life sciences. At the present time, computer simulations allow to understand complex ex- perimental data, and to rationalize or even predict the properties of molecules and solids as well as their reac- tions based on detailed structural and dynamical infor- mation at the atomic level. A fundamental requirement to perform such simulations is the knowledge about the atomic interactions, i.e., the potential energy and the forces, which in principle can be obtained by solving the Schr¨odinger equation. Unfortunately, such quantum me- chanical calculations are computationally very demand- ing, even if relatively efficient methods such as density functional theory (DFT) [1, 2] are used. Therefore, the accessible time and length scales of ab initio molecular dynamics (MD) simulations [3, 4], in which the energies and forces are determined by DFT for each visited atomic configuration, are limited to a few hundred atoms and tens to hundreds of picoseconds. The computational effort could be substantially reduced if the multidimensional function defining the relation be- tween the atomic positions and the potential energy, i.e., the potential energy surface (PES), would be directly ac- cessible. Founded on the Born-Oppenheimer approxi- mation of quantum mechanics [5], the PES contains a wealth of information, such as global and local minima defining stable and metastable geometries, barriers of chemical reactions, and forces governing the nuclear mo- tion. Approximate atomistic potentials and force fields representing the PES in analytic form have been used for decades [6–12], and conventional approaches rely on physically reasonable simplifications to increase the com-\n",
      "\n",
      "putational efficiency. Such approximations typically re- duce the accuracy but often still allow to maintain an acceptable transferability of the potential. In recent years, machine learning (ML) has emerged as a new and powerful computational tool with many ap- plications in the chemical and physical sciences [13–15], such as drug design [16, 17], synthesis planning [18, 19], protein structure prediction [20] and the analysis and pre- diction of spectra [21, 22]. Another important usage of ML is the representation of the PES by machine learning potentials (MLP), which has first been proposed more than a quarter of a century ago [23]. Since then, MLPs have witnessed tremendous progress [24–37] by exploit- ing the flexibility of ML methods such as neural networks (NNs) to learn the atomic interactions from reference en- ergies and forces obtained from electronic structure cal- culations. The resulting analytical ML expression can then provide the energy and its derivatives with about the accuracy of the reference method at a small fraction of the computational costs, which works well as long as the requested structures are not too different from those included in the training set. Due to this limited extrap- olation capabilities, often large and structurally diverse data sets are used in the construction of MLPs. Another advantage of MLPs is their ability to represent all types of bonding, such as covalent and metallic bonds as well as ionic and dispersion interactions, at the same level of accuracy by utilizing general and unbiased functional forms. Moreover, like the underlying electronic struc- ture methods, they are “reactive”, i.e., they can describe the making and breaking of bonds. However, the com- putational costs of MLPs are usually higher than those of simple classical force fields, since a large number of terms needs to be evaluated and more complex coordi- nate transformations are required. Current MLPs can be assigned to different genera- tions [36, 38], and a suitable choice depends on the spe- cific system of interest. While MLPs of the first gen- eration are restricted to low-dimensional systems, the introduction of second-generation high-dimensional neu- ral network potentials in 2007 paved the way to the\n",
      "\n",
      "∗ alea.tokita@ruhr-uni-bochum.de † joerg.behler@ruhr-uni-bochum.de\n",
      "\n",
      "application of MLPs to high-dimensional systems con- taining large numbers of atoms [38–41]. This has been achieved by expressing the potential energy of the sys- tem as a sum of atomic energies and by the introduc- tion of atom-centered symmetry functions (ACSFs) as atomic environment descriptors maintaining the manda- tory translational, rotational and permutational invari- ances of the energy [42]. Over the years, many types of second-generation MLPs have been introduced differ- ing in the employed ML algorithms and descriptors, such as various forms of neural network potentials [39, 43–46], Gaussian approximation potentials [47, 48], moment ten- sor potentials [49], spectral neighbor analysis potentials [50], atomic cluster expansion[51] and many others. Long-range electrostatic interactions are included in third-generation MLPs [52–56]. Here, the necessary charges or even multipoles can be predicted as a func- tion of the atomic environments by machine learn- ing [52, 53, 57, 58]. Also MLPs with explicit dispersion interactions beyond the local atomic environments can be classified as third-generation [59–61]. Fourth-generation MLPs [62–66], which employ global charge equilibration techniques [67] or self-consistent charge distributions [63], can describe non-local phenomena such as long-range charge transfer and can be applied to multiple charge states of a system. While many MLPs make use of predefined features to de- scribe environment-dependent atomic properties such as energies, charges, or electronegativities, in recent years different forms of message passing neural networks [68] have also been developed for the representation of PESs. Here, the description of the atomic environments is in- cluded in the training process by iteratively passing struc- tural information through the system [44, 45, 53, 69–71], which in principle can provide information beyond a pre- defined local environment. Despite the significant progress that has been made in the increasingly automated generation of MLPs in the past two decades [72–74], constructing MLPs is still not a black box task. For instance, it is important to be aware that MLPs can spectacularly fail as a consequence of their flexible functional form if they have not been carefully validated or if they are used beyond the range of structures they have been developed for. Therefore, the construction and the validation of MLPs should go hand in hand, and users need to know about the appli- cability and the limitations of a specific potential, which strongly depends on the underlying data set. Hence it is important to distinguish between the general capabilities of a MLP method and the performance of a specific pa- rameterization for a given system. This subtle difference poses a significant challenge for an unbiased comparison of different methodologies. In spite of the rapidly growing body of publications on MLPs and their applications, there are not many Tuto- rials in the literature [32, 75, 76] addressing these issues. With this Tutorial we aim to fill this gap by discussing in detail all important aspects of training and validat-\n",
      "\n",
      "InitialDataSet\n",
      "\n",
      "FeatureSelection\n",
      "\n",
      "IterativeNeuralNetworkPotentialTrainingValidation\n",
      "\n",
      "NeuralNetworkSettings\n",
      "\n",
      "DataSetExtensionWeightOptimization\n",
      "\n",
      "FinalPotentialActiveLearning:\n",
      "\n",
      "FIG. 1: Overview of the construction of a neural network potential.\n",
      "\n",
      "ing MLPs. As a real-life example, we will use high- dimensional neural network potentials (HDNNP) [39] representing a common type of MLP. Nevertheless, most of the discussion equally applies to different classes of neural network potentials as well as MLPs employing other types of machine learning algorithms. The general procedures are illustrated using a simple model system consisting of a LiOH ion pair in a small box of water. The overall workflow for training a neural network poten- tial is shown schematically in Fig. 1 and corresponds to the structure of this Tutorial. After a concise summary of the HDNNP methodology in Section II, the generation of the initial data set is discussed in Section III. Moreover, several decisions have to be made before the training pro- cess can be started, such as the selection of the features or descriptors and the settings of the neural networks, e.g. the architecture and activation functions. These prepa- rations for the training are presented in Section IV. The training procedure, which is usually an iterative process that also involves the extension of the data set by ac- tive learning, is then explained in Sec. V followed by a recipe for the validation of the potential in Sec. VI. The Tutorial ends with some final remarks and conclusions in Sec. VII. Some illustrative examples are given in the supplementary material.\n",
      "\n",
      "2\n",
      "\n",
      "II. HIGH-DIMENSIONAL NEURAL NETWORK POTENTIALS\n",
      "\n",
      "A. Method\n",
      "\n",
      "A major drawback of early, first-generation MLPs has been the limitation to a small number of atoms, which prevented their use in simulations of complex molecular and condensed systems. A strategy to address large sys- tems, which has been used in empirical potentials for a long time with great success is the decomposition of the total potential energy Etot of the system into structure- dependent atomic energies Ei, e.g., in the famous Tersoff potential [6] and the embedded atom method [7]. However, transferring this strategy to the realm of MLPs has been a frustrating task in the early days of MLP de- velopment. While the high dimensionality of machine learning algorithms is the reason for their superior ac- curacy, it also posed severe challenges for finding suit- able coordinates considering the mandatory invariances of the potential energy with respect to translation, rota- tion and permutation, i.e., the order of chemically equiv- alent atoms of the same element. The rotational and translational invariances can in prin- ciple be incorporated easily by using internal coordinates such as interatomic distances instead of the Cartesian positions. However, the output of machine learning al- gorithms such as neural networks (NN) depends on the order in which such internal coordinates are supplied in the vector of input values, which violates permutation invariance. Moreover, feed-forward NNs, which have al- most exclusively been used in early MLPs, have a fixed dimensionality, preventing the construction of PESs for systems with variable numbers of atoms. A solution to this problem has been found in 2007 with the introduction of atom-centered symmetry functions (ACSF) [39, 42] as atomic environment descriptors fulfill- ing all required invariances. These enabled the develop- ment of high-dimensional neural network potentials [39] applicable to systems consisting of thousands of atoms using the total energy expression\n",
      "\n",
      "Etot =\n",
      "\n",
      "Natoms(cid:88)\n",
      "\n",
      "Ei =\n",
      "\n",
      "Nelements(cid:88)\n",
      "\n",
      "Nj atoms(cid:88)\n",
      "\n",
      "Ej i\n",
      "\n",
      ".\n",
      "\n",
      "i=1\n",
      "\n",
      "j=1\n",
      "\n",
      "i=1\n",
      "\n",
      "As the atomic environments determining the atomic en- ergies are defined by a cutoff radius Rc, HDNNPs of this form represent a second-generation potential. Today, a very large number of atomic environment descriptors [77– 81] and many flavors of very accurate high-dimensional MLPs are available offering access to large-scale atom- istic simulations of condensed systems. The structure of a HDNNP is shown in Fig. 2 for the example of a LiOH ion pair in water. Starting from the Cartesian coordinate vectors Ri of the atoms first a transformation to the ACSF vectors Gi is carried out In general, the number and definition for each atom.\n",
      "\n",
      "(1)\n",
      "\n",
      "of the ACSFs (cf. Sec. IIB) can be different for each element. These vectors then enter the respective atomic feed-forward NNs (see Box 1), which are the central com- ponents of the HDNNP providing the atomic energies yielding Etot. For a given element, the architecture and weight param- eters of the atomic NNs are constrained to be the same. Consequently, Nelements different feed-forward NNs need to be trained simultaneously, and in applications of the HDNNP each element-specific NN is evaluated once per each atom of the respective element. Apart from the ele- ment information and the atomic positions – and if appli- cable the lattice vectors – no further structural informa- tion about the system is required for second-generation MLPs. Moreover, in contrast to most traditional force field no further classification of atoms of a given element into atom types depending on the local bonding patterns is required.\n",
      "\n",
      "Box 1: Feed-forward neural networks\n",
      "\n",
      "Multilayer feed-forward neural networks [82, 83], often also called “deep neural networks”, are a type of artificial neural network whose functional form is inspired by the interaction of neurons in the brain [84]. While initially they were used to develop mathematical models of these inter- actions, nowadays they have become a powerful technique in the field of machine learning to estab- lish functional relations between input and target properties [82, 85]. An example of a feed-forward neural network is shown in Fig. 3. As its name implies, the flow of in- formation is from left to right, i.e., from the input to the output layer along the black lines connect- ing the neurons, or nodes, in the network, which are represented by the circles. The structural in- formation is given by the vector G = {Gi} and is provided in the input layer. It is then passed via one or more hidden layers to the node(s) in the output layer. Each hidden layer contains several neurons, which are connected to the neurons in the neighboring layers by weight parameters akl ij representing the connection strength between node i in layer k and node j in layer l = k+1. In addition, each neuron j in layer l, which may be a hidden or the output layer, is connected to a bias node by a bias weight bl j acting as an adjustable offset. The numerical value yl j of a node j in layer l is given by a linear combination of the values of all neurons yk in the previous layer weighted by the i respective connecting weight parameters. Then, the bias weight is added and a non-linear activa-\n",
      "\n",
      "3\n",
      "\n",
      "1\n",
      "\n",
      "r\n",
      "\n",
      "u\n",
      "\n",
      "1\n",
      "\n",
      "L\n",
      "\n",
      "e\n",
      "\n",
      "1\n",
      "\n",
      "H\n",
      "\n",
      "r\n",
      "\n",
      "1\n",
      "\n",
      "1OE...\n",
      "\n",
      "1\n",
      "\n",
      "t\n",
      "\n",
      "I\n",
      "\n",
      "e\n",
      "\n",
      "d\n",
      "\n",
      "y\n",
      "\n",
      "GKLi\n",
      "\n",
      "1\n",
      "\n",
      "p\n",
      "\n",
      "1\n",
      "\n",
      "H\n",
      "\n",
      "p\n",
      "\n",
      "d\n",
      "\n",
      "1\n",
      "\n",
      "L\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "u\n",
      "\n",
      "n\n",
      "\n",
      "r\n",
      "\n",
      "n\n",
      "\n",
      "2\n",
      "\n",
      "n\n",
      "\n",
      "e\n",
      "\n",
      "d\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "i\n",
      "\n",
      "e\n",
      "\n",
      "a\n",
      "\n",
      "Y\n",
      "\n",
      "Y\n",
      "\n",
      "G1OG2OI\n",
      "\n",
      "2\n",
      "\n",
      "Y\n",
      "\n",
      "1\n",
      "\n",
      "G\n",
      "\n",
      "1\n",
      "\n",
      "a\n",
      "\n",
      "Y\n",
      "\n",
      "e\n",
      "\n",
      "y\n",
      "\n",
      "O\n",
      "\n",
      "1\n",
      "\n",
      "L\n",
      "\n",
      "n\n",
      "\n",
      "Y\n",
      "\n",
      "d\n",
      "\n",
      "Y\n",
      "\n",
      "1\n",
      "\n",
      "R1HR2HR3HR4HRNHR1ORMOR1Li\n",
      "\n",
      "O\n",
      "\n",
      "e\n",
      "\n",
      "GIH123N...\n",
      "\n",
      "a\n",
      "\n",
      "L\n",
      "\n",
      "y\n",
      "\n",
      "n\n",
      "\n",
      "e\n",
      "\n",
      "GJO1M\n",
      "\n",
      "d\n",
      "\n",
      "G\n",
      "\n",
      "G1HG2HE1HInput LayerHidden Layer 1Hidden Layer 2OutputLayerY\n",
      "\n",
      "r\n",
      "\n",
      "Y\n",
      "\n",
      "t\n",
      "\n",
      "a\n",
      "\n",
      "d\n",
      "\n",
      "H\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "Etot\n",
      "\n",
      "a\n",
      "\n",
      "1\n",
      "\n",
      "e\n",
      "\n",
      "r\n",
      "\n",
      "y\n",
      "\n",
      "i\n",
      "\n",
      "L\n",
      "\n",
      "1\n",
      "\n",
      "G1LiG2LiE1LiY\n",
      "\n",
      "i\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "1\n",
      "\n",
      "y\n",
      "\n",
      "FIG. 2: Schematic structure of a second-generation high-dimensional neural network potential (HDNNP) [39] for the example of a lithium hydroxide ion pair in water (lithium: dark blue; oxygen: red; hydrogen: grey). First, the\n",
      "\n",
      "Cartesian coordinate vectors Rα\n",
      "\n",
      "i of the atoms i of element α are transformed to vectors of j atom-centered j [42], which describe the local atomic environments up to the cutoff radius shown as\n",
      "\n",
      "symmetry functions (ACSF) Gα\n",
      "\n",
      "circle for the case of the lithium atom. For each atom the respective ACSF vector is then used as input for an\n",
      "\n",
      "atomic neural network (NN) predicting its atomic energy Eα i . Finally, the atomic energies are summed to obtain the total potential energy Etot of the system (Eq. 1). The sets of ACSFs, the architectures of the atomic NNs, and the weight parameters are the same for all atoms of a given element.\n",
      "\n",
      "tion function fl\n",
      "\n",
      "j is applied yielding\n",
      "\n",
      "j = fl yl j\n",
      "\n",
      " bl\n",
      "\n",
      "j +\n",
      "\n",
      "Nk nodes(cid:88)\n",
      "\n",
      "ij · yk akl i\n",
      "\n",
      "\n",
      "\n",
      " .\n",
      "\n",
      "i=1\n",
      "\n",
      "Consequently, the output E of the small feed- forward neural network in Fig. 3 is given by\n",
      "\n",
      "E = f3 1\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "b3 1 +\n",
      "\n",
      "4 (cid:88)\n",
      "\n",
      "k1 · f2 a23 k\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "b2 k\n",
      "\n",
      "(2)\n",
      "\n",
      "to avoid restricting the output values of the feed- forward neural network. Alternatively, the target values can be (re)scaled. The flexibility of a network is determined by its i.e., the number of hidden layers architecture, and nodes per layer. The more layers and nodes are included, the larger are the fitting capabilites of the network, which often contain a few thou- sand weights. Typically, the feed-forward NNs in HDNNPs consist of two to three hidden layers in- cluding between 15 and 50 neurons each.\n",
      "\n",
      "+\n",
      "\n",
      "4 (cid:88)\n",
      "\n",
      "k=1 (cid:32)\n",
      "\n",
      "jk · f1 a12 j\n",
      "\n",
      "b1 j +\n",
      "\n",
      "3 (cid:88)\n",
      "\n",
      "a01 ij · Gi\n",
      "\n",
      "(cid:33)(cid:33)(cid:33)\n",
      "\n",
      "j=1\n",
      "\n",
      "i=1\n",
      "\n",
      "Activation functions are an important compo- nent of a neural network, since they introduce non-linearity to the model and thereby give feed- forward neural networks the ability to fit arbitrary non-linear functions [86, 87] such as potential en- ergy surfaces. Various functional forms can be used such as the hyperbolic tangent, the sigmoid function, and Gaussians. They have in common that they contain a non-linear region, and are con- tinuous and differentiable, which is needed for the gradient-based optimization of the network and for the computation of atomic forces in MLPs. For the output layer usually a linear function is used\n",
      "\n",
      ".(3)\n",
      "\n",
      "G311a1211a01b11b21b3111a23\n",
      "\n",
      "G1G2EInput LayerHidden Layer 1Hidden Layer 2OutputLayery11Bias\n",
      "\n",
      "y21y31y41y12y22y32y42\n",
      "\n",
      "4\n",
      "\n",
      "FIG. 3: Architecture of a feed-forward neural net- work. The output energy E is a function of the neurons {Gi} in the input layer. In between the input and the output layer the neurons are ar- ranged in hidden layers and determine the func- tional flexibility of the neural network. The black lines connecting pairs of neurons and also dotted lines between the bias node and the neurons repre- sent the fitting weight parameters of the network.\n",
      "\n",
      "B. Atom-Centered Symmetry Functions\n",
      "\n",
      "The availability of suitable descriptors for the atomic environments is a key for the construction of high- dimensional MLPs. Here, we will just briefly summarize two types of atom-centered symmetry functions, which are most commonly chosen when constructing HDNNPs, but in principle many other types of descriptors could equally be used. The spatial extension of the ACSFs as a function of distance Rij of atom j from central atom i is defined by a cutoff function such as the monotonously decreasing part of the cosine function,\n",
      "\n",
      "fc (Rij) =\n",
      "\n",
      "(cid:40)\n",
      "\n",
      "0.5 ·\n",
      "\n",
      "(cid:104)\n",
      "\n",
      "cos\n",
      "\n",
      "(cid:16)πRij Rc\n",
      "\n",
      "0.0\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "+ 1\n",
      "\n",
      "(cid:105)\n",
      "\n",
      "for Rij ≤ Rc for Rij > Rc ,\n",
      "\n",
      "which smoothly decays to zero in value and slope at Rc (cf. Fig. 4a). There are two main classes of ACSFs termed radial and angular, which can be used to provide local structural fingerprints of the atomic environments. The most common radial ACSF has the form\n",
      "\n",
      "Grad\n",
      "\n",
      "i =\n",
      "\n",
      "Natoms∈Rc(cid:88)\n",
      "\n",
      "e−η(Rij−Rs)2\n",
      "\n",
      "fc (Rij)\n",
      "\n",
      "j=1\n",
      "\n",
      "and consists of a set of Gaussian functions (see Fig. 4b) evaluated at the radial distances of all neighboring atoms inside the cutoff sphere. To ensure that the number of symmetry functions is independent of the number of neighbors, which is required for compatibility with the fixed dimensionality of the input vectors of the atomic NNs, the Gaussian functions are multiplied by the cutoff function and the sum of all terms is calculated to yield a single value, which can be interpreted as an effective coor- dination number within a range defined by the Gaussian width parameter η. The parameter Rs allows to shift the centers of the Gaussians away from central atom i to increase the sensitivity of the function in specific spher- ical shells. To obtain a radial profile of the neighboring atoms, a set of radial symmetry functions is constructed employing element pair-specific sets of different η values (see Sec. IV). Angular symmetry functions consider the angles θijk of triplets of atoms centered between the connections ij and ik employing the cosine function such that the periodicity\n",
      "\n",
      "(4)\n",
      "\n",
      "(5)\n",
      "\n",
      "of the angle and in particular its mandatory symmetry with respect to angles of 0◦ and 180◦ are taken into ac- count,\n",
      "\n",
      "Gang\n",
      "\n",
      "i =21−ζ\n",
      "\n",
      "Natoms∈Rc(cid:88)\n",
      "\n",
      "Natoms∈Rc(cid:88)\n",
      "\n",
      "[(1 + λ · cosθijk)ζ\n",
      "\n",
      "j̸=i ij+R2\n",
      "\n",
      "k̸=i,j\n",
      "\n",
      "e−η(R2\n",
      "\n",
      "jk) · fc (Rij) · fc (Rik) · fc (Rjk)] (6)\n",
      "\n",
      "ik+R2\n",
      "\n",
      "There are two parameters, which can be varied to char- acterize the atomic environment. These are the values of ζ and λ. As can be seen in Fig. 4c, the purpose of λ = +1 or λ = −1 is to center the maxima of the cosine function at angles of 0◦ or 180◦, respectively, while a series of ζ values controls the angular resolution of the functions. Multiplying the angular terms by the three cutoff func- tions of the three pairwise distances ensures that only triples of close atoms enter the summation, while the Gaussian functions can be used to contract the spatial range to close neighbors, if desired. This can be useful since for a given angle the distance between neighbors j and k increases with Rij and Rjk resulting in weaker interactions. It is important to note that there is no need for a linear re- lation between the values of the ACSFs and the potential energy, since the atomic NNs are able to express very gen- eral functional forms. Therefore, only some mandatory properties of the ACSFs are relevant for the construction of HDNNPs (see Box 2). For each element in the system, its radial and angular functions are constructed for all element combinations present for atoms j (radial func- tions) or j and k (angular functions), respectively, to re- flect the different interactions between different chemical species. Typically, between 10 and 200 ACSFs are used per atom depending on the complexity of the system. Although ACSFs consist of atomic distances and angles, they simultaneously depend on all atomic positions inside the cutoff sphere. Hence, they are formally many-body functions. Still, it has been shown that in rare situa- tions descriptors consisting of two- and three-body terms may not be unique in particular for low-dimensional sys- tems [88]. A more detailed discussion and further types of ACSFs can be found in Ref. 42.\n",
      "\n",
      "Box 2: Properties of ACSFs\n",
      "\n",
      "To be suitable descriptors for the atomic environ- ments, atom-centered symmetry functions must fulfill several requirements.:\n",
      "\n",
      "They need to describe the structural details of the atomic environments.\n",
      "\n",
      "Their values must be invariant with respect to rotation, translation and permutation.\n",
      "\n",
      "They have to decay smoothly to zero in value\n",
      "\n",
      "5\n",
      "\n",
      ".\n",
      "\n",
      "and slope at the cutoff radius to restrict the atomic interactions to the environments in- side the cutoff spheres.\n",
      "\n",
      "They need to be continuous and differ- entiable for the gradient-based training of HDNNPs and the calculation of analytic forces.\n",
      "\n",
      "The number of ACSFs in the atomic NN in- put vectors must be independent of the num- ber of neighbors inside the cutoff sphere, be- cause this number can be different for each atom and can change in MD simulations.\n",
      "\n",
      "C. Long-Range Electrostatic Interactions\n",
      "\n",
      "So far, we have discussed the structure of second- generation HDNNPs and ACSFs as descriptors for the training of environment-dependent atomic energies. The main assumption of such second-generation potentials is the locality of a major part of the atomic interactions, which, if valid, can be expressed to a good approximation in terms of local atomic energies. For clarity it should be noted that even second-generation MLPs contain all types of interactions, including electrostatics and disper- sion, between atoms within the cutoff radius, since Eq. 1 does not distinguish between different types of bonding. However, while this ansatz works surprisingly well for many systems, in some cases it is necessary to explic- itly include long-range interactions such as electrostatics without truncation [89]. The inclusion of long-range electrostatic interations is possible using, e.g., an Ewald sum [90] employing atomic charges determined by ML. The extended total energy expression is then given by\n",
      "\n",
      "Etot = Eelec + Eshort = E({Qi},{Ri}) +\n",
      "\n",
      "Natoms(cid:88)\n",
      "\n",
      "Ei({Ri})(7)\n",
      "\n",
      "i=1\n",
      "\n",
      "as a sum of a short-range and a long-range part. There are different options to obtain the required charges. If the atomic charges are essentially local, i.e., they only depend on the close chemical environment, they can be learned as atomic properties using ACSF descriptors and a second set of atomic neural networks, e.g., in third- generation HDNNPs [52, 91]. If, however, the charges depend on distant structural features and are influenced by long-range charge transfer, such as in aromatic sys- tems or molecules containing conjugated π-bonds, fourth- generation potentials may be required [64]. In this case, the charges are determined indirectly via environment- dependent atomic electronegativities in combination with a charge equilibration step or a self-consistent redistribu- tion of the charges. Most of the aspects of training neural network potentials\n",
      "\n",
      "are not much affected by the decision if a second-, third-, or fourth-generation HDNNP is trained, since the proce- dures for the selection of the data, the iterative training and the validation are essentially the same. Therefore, in this Tutorial we will focus on the example of a second- generation HDNNP, while some comments on the par- ticularities of training HDNNPs including electrostatic interactions are given in Section VC. Further details on third- and fourth-generation HDNNPs can be found in Refs. 36, 38, 52, 64, 91.\n",
      "\n",
      "III. DATA SET GENERATION\n",
      "\n",
      "A. Reference Electronic Structure Calculations\n",
      "\n",
      "Before a MLP can be trained, first a reference elec- tronic structure method has to be chosen. When con- structing potentials for condensed systems, often DFT calculations are used. These can be carried out routinely for systems containing a few hundred atoms with and without periodic boundary conditions even if thousands of data points are required, which is typical for MLP training. However, also coupled cluster and other wave function-based methods are increasingly popular [92, 93], since any limitation in the accuracy of the training data will become an intrinsic property of the MLP, which thus cannot provide more accurate results than the underlying reference method. Still, due to the high computational costs, often a compromise between accuracy and com- putational efficiency has to be found when choosing the reference method. It is thus important to note that the choice of the reference method determines the reliability of the final MLP, i.e., the agreement with experimental observables, and the only aim of MLP construction is to reproduce this method with all its intrinsic properties. Ideally, the reference method should not only provide accurate energies but also atomic forces, which contain a wealth of information about the local shape of the PES. Including forces in the training, which is good prac- tice in almost any modern MLP [47, 75, 94–96], offers several advantages. First, the amount of information that can be extracted from expensive electronic structure calculations is dramatically increased, since each single point calculation can only provide one energy value but 3Natoms force components. Moreover, as has been shown recently for the case of metal-organic frameworks [97], MLPs can be subject to arbitrary internal energy shifts between the individual atoms. These internal energy shifts, which can strongly reduce the transferability of MLPs, are invisible when monitoring the total energy er- rors in the training process, because there are many ways to distribute the total energy in the system for a given total energy. Forces, which can provide local information about the gradients of the PES at the atomic positions, can contribute to overcome these problems. The generation of the reference data set is often the com- putational bottleneck in the construction of MLPs as\n",
      "\n",
      "6\n",
      "\n",
      "= 0.021\n",
      "\n",
      "0.8\n",
      "\n",
      "Rc= 6.0\n",
      "\n",
      "0.0\n",
      "\n",
      "10Rij[a0]\n",
      "\n",
      "= 0.052\n",
      "\n",
      "0.4\n",
      "\n",
      "0.2\n",
      "\n",
      "= 0.168\n",
      "\n",
      "0\n",
      "\n",
      "0.4\n",
      "\n",
      "1.0gradij\n",
      "\n",
      "Rc= 10.0\n",
      "\n",
      "= 0.0\n",
      "\n",
      "5\n",
      "\n",
      "10Rij[a0]\n",
      "\n",
      "1.0fc\n",
      "\n",
      "Rc= 8.0\n",
      "\n",
      "0.6\n",
      "\n",
      "0\n",
      "\n",
      "0.8\n",
      "\n",
      "0.6\n",
      "\n",
      "= 0.006\n",
      "\n",
      "Rc= 12.0\n",
      "\n",
      "0.0\n",
      "\n",
      "0.2\n",
      "\n",
      "Rc= 11.0\n",
      "\n",
      "a)b)c)λ = 1λ = -1ζ = 16ζ = 1ζ = 2ζ = 4\n",
      "\n",
      "5\n",
      "\n",
      "FIG. 4: Different types and components of atom-centered symmetry functions (ACSF). Panel a) shows the cutoff\n",
      "\n",
      "ij = e−η(Rij−Rs)2 function fc(Rij) (Eq. 4) for different cutoff radii Rc (in a0). Panel b) displays a term grad ·fc(Rij) of the radial symmetry function in Eq. 5 for different Gaussian exponents η (in a−2 0 ) with Rc = 12 a0 and Rs = 0 a0. In panel c) the dependence of the angular symmetry functions (Eq. 6) on the angle θijk and the distance Rij as radial coordinate is displayed for a cutoff of Rc = 12 a0 and the cases λ = +1 and λ = −1 with different ζ parameters.\n",
      "\n",
      "they usually consist of tens of thousands of data points. This effort can be reduced in two ways: by carefully choosing and thus decreasing the number of structures, which is a very active field of research (see Sec. VB), and by reducing the costs of each calculation by optimizing the settings of the employed electronic structure code. The numerical convergence of the electronic structure calculations is a crucial point, since any numerical noise in the data represents a limit to the minimum error that can be reached in the training process. Particu- larly problematic is the choice of k-points if periodic sys- tems of different size or symmetry are combined in a sin- gle training set, because underconverged k-point grids can have a significant impact on the accuracy of ener- gies and forces, which finally results in inconsistent data in the reference set (see Fig. 5a). Attention should also be paid to the density of Furthermore numerical inte- gration grids, which usually have a fixed orientation in If too sparse grids are used, data sets space [98, 99]. with different molecular orientations might contain simi- lar structures with contradictory energies and forces (see Fig. 5b). While this is straightforward to detect for small gas phase molecules, in condensed systems such as liquid water, which contain many similar bonds in multiple dif- ferent directions, such inaccuracies might be extremely difficult to detect resulting in poor potentials. In general, a numerical convergence to about 1 meV/atom for energy differences and 0.05–0.10 eV/˚A for the forces should be considered as a minimum requirement. Finally, there are other properties next to energies and forces, which might be needed for the construction MLPs\n",
      "\n",
      "and should be computed and stored. Apart from atomic partial charges for MLPs containing electrostatic interac- tions, this might be information about the spin for mag- netic systems [100–102] or Hirshfeld volumes for disper- sion interactions [60]. Sometimes also the stress tensor is used for training [103].\n",
      "\n",
      "B. System Size and Dimensionality\n",
      "\n",
      "Once the decision concerning the electronic structure method has been made and converged settings have been determined, the required structure types have to be se- lected. This includes many aspects, such as the system size, the chemical composition and the structural diver- sity, because MLPs have limited extrapolation capabili- ties beyond the range of structures included in the train- ing set. Unfortunately, with the exception of smallest molecules in vacuum, mapping atomic configurations on a regular grid is not a feasible option due to the expo- nential growth in the amount of possible structures with increasing number of atoms. energy as a sum the construction of The of environment-dependent atomic energies in second- generation MLPs corresponds to an effective reduction of the dimensionality for large condensed systems since only the positions of the atoms inside the cutoff spheres need to be learned while the total energy of the system still explicitly depends on all atomic positions. Fig. 6 shows the number of neighboring atoms included in the local environment for cutoff radii between 4 and 7 ˚A for\n",
      "\n",
      "total\n",
      "\n",
      "7\n",
      "\n",
      "xy\n",
      "\n",
      "a)b)RealSpaceSparseGridDenseGridxyReciprocalSpacek-pointsk-PointsEIntegrationGrids\n",
      "\n",
      "xy\n",
      "\n",
      "peratom(2x2)\n",
      "\n",
      "E1E2\n",
      "\n",
      "E1E2\n",
      "\n",
      "xy\n",
      "\n",
      "a1\n",
      "\n",
      "aa2a2atoms1E2\n",
      "\n",
      "FIG. 5: Numerical accuracy of integration grids in electronic structure calculations. Panel a) shows the selection of consistent k-point grids in periodic DFT calculations when changing the cell size. For supercells it is sometimes possible to choose exactly equivalent k-point meshes in reciprocal space, e.g., when the lattice parameter a is doubled and the number of k-points in the corresponding direction can be devided by two. For the combination of arbitrary systems of different size in the training set the k-point density should be kept constant. Panel b) illustrates the role of numerical integration grids for the case of a diatomic molecule, which might, e.g., be atom-centered grids used for the representation of numerical atomic orbitals or electron densities. For a sparse grid, the energy of the system does not show the required rotational invariance (e.g. in representing the grey region), while for a dense grid this invariance is to a good accuracy achieved. The same effect can be observed for regular three-dimensional grids used, e.g., in Fourier transformations.\n",
      "\n",
      "the examples of liquid water and fcc bulk copper. As can be seen, typical environments contain between 50 and 150 atoms. These atoms define the configuration space that should be mapped for the generation of the MLP, which requires a system size of at least the extension of the cutoff sphere. In case of periodic systems, the small- est cell diameter should be larger than the diameter of the cutoff sphere to avoid sampling only a subspace of the possible atomic configurations due to an artificial pe- riodicity of the system. Thus, training structures for con- densed systems often reach a size between 150 and 250 atoms. Still, it is possible to combine systems of different size in the training set to benefit from faster calculations for smaller systems, as long as consistent settings are used (see Sec. IIIA). We note here that also MLPs of the third and fourth generation follow the same princi- ple of environment-dependent properties such that also for these generations in principle it is not necessary to significantly increase the size of the reference structures. However, the choice of the cutoff is a critical decision and will be discussed in Sec. IVB. Even if the effective dimensionality is reduced due to the\n",
      "\n",
      "cutoff sphere, a comprehensive sampling is still out of reach and systematic strategies to select the most impor- tant data points are needed (see Sec. VB). Unphysical atomic configurations, such as too close atomic encoun- ters or chemically unreasonable bonding patterns should be avoided, as they would have very high potential en- ergies and would be irrelevant in atomistic simulations. Still, any moderate and thus chemically still relevant in- crease in temperature enlarges the size of accessible con- figuration space and makes the mapping more demand- ing. For some condensed and molecular systems it is also possible to reduce the complexity of the reference sys- tems by making use of molecular fragments, which can be extracted from the system [95, 97, 104, 105].\n",
      "\n",
      "C. Generation of an Initial Data Set\n",
      "\n",
      "Before a first MLP can be trained, an initial data set needs to be available, which – depending on the complex- ity of the system – might contain a few hundred up to a few thousand structures. Ideally, the atomic configu-\n",
      "\n",
      "8\n",
      "\n",
      "Watera)4 5 6 7204069109275490144b)CopperNumber of atoms60120207327Cutoff [Å]Number of configurations\n",
      "\n",
      "Cutoff [Å]Number of atomsNumber of configurations\n",
      "\n",
      "4 5 6 7\n",
      "\n",
      "FIG. 6: Structural complexity of atomic environments. Panels a) and b) show typical numbers of atoms inside the cutoff spheres for different cutoff radii Rc for bulk liquid water and fcc bulk copper at ambient conditions. The number of possible atomic configurations inside the cutoff spheres increases strongly with the effective dimensionality of the atomic environments.\n",
      "\n",
      "rations should be chemically reasonable and as close as possible to the geometries visited in the intended applica- tions of the potential. They should therefore be sampled at the corresponding conditions, such as temperature, pressure, and chemical composition. A straightforward although expensive way of generat- ing reasonable initial data sets is the use of ab initio MD. Here, often less tight settings are used, such as Γ- point sampling of k-space only. Therefore, energies and forces from ab initio MD trajectories are often not used directly but provide geometries, which are then recom- puted with a higher degree of convergence or even more demanding electronic structure methods. This also al- lows to avoid the computation of structurally correlated geometries such as configurations visited in subsequent MD steps, which only provide a marginal amount of new information, by selecting structures in specific intervals. A significant advantage of ab initio MD is the generation of reasonable structures already right in the beginning of the data set construction. Still, it should be taken into account that potential energy wells are more frequently sampled than repulsive high-energy regions or transition states. The representation of repulsive walls can be im- proved by running simulations at slightly elevated tem- peratures and pressures, which increase the probability of closer atomic encounters. Transition states can be in- cluded using, e.g., metadynamics [106]. If available for the system of interest, first structures for electronic structure calculations can also be extracted It should be from conventional force field trajectories. noted, however, that different equilibrium geometries may result in structural biases, which – to a much smaller extent – can also be present in ab initio MD simulations\n",
      "\n",
      "when using less converged settings or different function- als. There are several alternative approaches to generate pre- liminary data sets. In particular in the field of materials science it is common to start from the known crystal structures of a material and to introduce step-by-step random atomic displacements of increasing amplitude to sample the local minimum wells. In molecular systems, also normal mode sampling is frequently used [43, 107]. Furthermore, randomly placed atoms and molecules can be used in combination with suitable interatomic dis- tance constraints. Finally, also the increasing availabil- ity of repositories and databases might be an interesting source of initial data sets [108, 109]. However, the initial data set represents only a first step, and its extension by further consistent data is crucial, which makes the reuse of repository data difficult, as the exact input settings need to be known and the same version of the same elec- tronic structure code has to be available (see Sec. VB).\n",
      "\n",
      "Case study: Construction of a first data set for the LiOH-water system\n",
      "\n",
      "For the DFT calculations of the lithium hydroxide system the Car-Parrinello Projector Augmented Wave (CP-PAW) code (version 28-09-2016) was employed [110] using the PBE0r local hybrid functional [111] employing the settings described in Ref. 112. CP-PAW implements the projector augmented-wave (PAW) method [113], in which augmentations of the 1s orbital of H, the 2s and 2p orbitals of Li, and the 2p, 2p, and 3d orbitals\n",
      "\n",
      "9\n",
      "\n",
      "of O were used. The auxiliary wave functions were set up as node-less partial waves. Their matching radii for all orbitals are 0.7 times the covalent radii, which are 0.32 ˚A for H, 1.23 ˚A for Li, and 0.73 ˚A for O. The plane wave cutoffs of the auxiliary wave functions and the auxiliary density have been set to 35 EH and 140 EH, respectively. In the PBE0r tight binding orbitals used for the calculation of the Hartree-Fock exchange, contributions of the 1s orbital of H, the 2s orbital of Li, and the 2s and 2p orbitals of O were incorporated. For cubic cells with a lattice constant of 8.2 ˚A a 2×2×2 k-point grid was chosen. k-point grids of similar k-point density were selected for other cell sizes. Wave function and geometry optimizations were carried using the Car-Parrinello ab initio molecular dynamics method [3] including a friction term. The total energy was converged to out up to a numerical convergence of 10−5 EH. The D3 dispersion correction [114] was applied by the DFTD3 software (version 14-06-2016), which uses Becke-Johnson damping.\n",
      "\n",
      "lithium system contains The periodic ion, one hydroxide ion and between 19 to 23 solvating water molecules. We note that these systems are too small to provide a realistic description of general LiOH solutions and have been chosen for demonstration purposes in this Tutorial only. To construct an initial data set, at first the lithium ion, the hydroxide ion, and the water molecules were placed randomly in small boxes. The dimensions of the box were chosen to yield densities in the range between 0.97 and 1.02 kgdm−3. Structures were checked and only accepted if atom-atom distances were larger specific minimum distance than element-pair thresholds. These thresholds are 1.2 ˚A for H to H, 0.85 ˚A for H to O and H to Li, and 2.3 ˚A for O to O.\n",
      "\n",
      "one\n",
      "\n",
      "Initially, the structures have been optimized. Then, ab initio MD simulations with different simulation temperatures ranging from 100 K to 500 K were set up. After 5000 equilibration time steps every 1000 time steps the configurations were saved. In this way an initial data set of 630 reference structures was created. To this first data set an existing set of pure water structures reported in Ref. 115 was added.\n",
      "\n",
      "IV. PREPARATIONS FOR THE TRAINING\n",
      "\n",
      "A. Preparing the Atomic Neural Networks\n",
      "\n",
      "With an initial data set at hand, several further choices have to be made before the training process can begin. First of all, the architectures of the individual element- specific atomic neural networks have to be chosen. Often, several architectures differing in size are used for training, because a priori it is not clear, which NN architecture, i.e., which number of hidden layers and neurons per hid- den layer, will be best suited for representing the data set. Moreover, the non-linear activation functions have to be selected. This process will then generate a set of different HDNNPs, which are also needed for the itera- tive improvement of the data set by active learning (see Sec. VB). In principle, for each element a different NN architecture could be defined, but for simplicity it is common to use the same architecture for all elements, which typically consists of two – sometimes three – hidden layers, each of which contains between 15 and 50 neurons per layer. Therefore, compared to other applications in data sci- ence, the atomic NNs in HDNNPs are relatively small and can be quickly evaluated. The atomic NNs are usu- ally standard, i.e., fully connected, feed-forward NNs, but also modifications such as direct links from the input to the output layer or omitted connections are possible. When choosing the architecture, care should be taken that the number of weight and bias parameters defined in this way is smaller than the information content of the training set. Although the atomic NNs typically contain a few thousand parameters for each element, this is usu- ally not a problem, if also the wealth of information in- cluded in the forces is used for training. Another point to be considered regarding the size of the atomic NNs is the increasing flexibility as a consequence of the growing number of parameters in larger atomic NNs. If the NNs are too small, the flexibility is insuffi- cient and the HDNNP will not be able to resolve all the details of the PES even if this information is present in the data set, which is called “underfitting”. If the NNs are too large, “overfitting” will occur resulting in a poor prediction quality for structures not included in the train- ing set. A more detailed description of the detection of overfitting and the selection of NN architectures is given in Sec. VA.\n",
      "\n",
      "B. Atom-Centered Symmetry Functions\n",
      "\n",
      "Next, the atom-centered symmetry functions have to be chosen. As explained in section IIA the purpose of the descriptors is to provide a structural fingerprint of the environments inside the cutoff spheres while being invariant with respect to permutational, translational and rotational symmetry. In contrast to the atomic NN architectures, the ACSFs are usually not the same for\n",
      "\n",
      "10\n",
      "\n",
      "each element, because the typical bond distances between atoms of different elements depend on the specific chem- ical species. First, the cutoff radius defining the atomic environments has to be selected. It represents a convergence parame- ter that has to be increased until all important atomic interactions are included. For a too small cutoff, atoms outside the cutoff sphere are still significantly interact- ing with the central atoms, which is equivalent to noise in the data limiting the accuracy that can be reached, since no information about the positions of the atoms If outside the cutoff sphere is available in the ACSFs. the cutoff is too large, the construction of the MLP be- comes more demanding, since a large configuration space in the atomic environments has to be mapped by the ref- erence electronic structure calculations. Consequently, for large cutoffs the effort to be invested in the reference calculations increases with respect to both, the number of structures and the size of the reference systems. More- over, the detailed representation of the atomic positions inside the cutoff spheres requires a larger number of de- scriptors, which increases the computational costs of the HDNNP energy and force prediction. Several procedures to determine the size of the cutoff ra- dius, which is required for a certain level of convergence of the forces acting on the central atom, have been sug- gested in the literature. Since atomic interactions beyond the cutoff introduce noise in the forces, the variance of the forces when freezing the atoms inside and moving atoms outside the cutoff sphere can be used for uncer- tainty quantification [47, 116]. Only if this uncertainty is small, the cutoff is large enough. Convergence tests of the forces as a function of the environment have also been reported in Ref. 95. An alternative approach, in which the individual interaction strength with each neighboring atom can be determined, is based on the Hessian [117], which provides the derivative of the atomic forces with re- spect to all atomic positions in the system. This method is also applicable to crystalline environments since small net forces may also be a consequence of compensating interactions in symmetric environments. Interestingly, as discussed in Refs. 41 and 105, the an- alytic forces of second-generation MLPs have twice the spatial range of the cutoff spheres defining the atomic en- ergy contributions. Since the forces in MLPs can be con- structed as the analytic derivatives of the atomic energies (Eq. 1), it is therefore possible to use reference systems, which are large enough to learn the atomic energies but too small to provide forces of a condensed phase envi- ronment. This has been demonstrated recently for the benchmark case of metal-organic frameworks [105] and can, if carefully tested, reduce the costs of the electronic structure calculations for the training set in that only in- formation from small systems is used, while the obtained potentials can predict correct forces beyond the training range. Once the cutoff has been determined, which is typically between 5 and 8 ˚A, the next step is the selection of the\n",
      "\n",
      "parameters of the ACSFs. Overall, there are two main strategies for the determination of these parameters. The first strategy aims for a balanced and systematic descrip- tion of space in the atomic environments, similar to ba- sis sets in electronic structure calculations. The second strategy is data-centered and aims to identify the best set of ACSFs for a given data set. This is motivated by the typically very inhomogenous distribution of atoms in the cutoff spheres, which is governed by chemical bonds and thus does not require the capability to describe all imaginable geometries. Since the reference data sets employed in the construc- tion of the potential are not static but are continuously extended by active learning (see Sec. VB), often a combi- nation of both strategies is used. In early stages unbiased and very general “default” sets of ACSFs (see Box 3) are generated following the first strategy, while for large and converged data sets finally the optimum set of ACSFs is used according to the second strategy to yield the small- est number of descriptors providing the desired accuracy. These can be selected in different ways, such as optimiz- ing the ACSF parameters by genetic algorithms [118], principal component analysis [119], or selecting functions from large pools of candidates based on CUR decomposi- tion or the Pearson correlation in an automatic way [120]. Different ACSFs can have very different ranges of values. It is therefore common to rescale the ACSF values be- fore using them as input for the atomic NNs [121]. This can be done by subtracting the mean value and adjusting the standard deviation of the symmetry functions of the reference data set. This procedure has to be repeated whenever new structures are included in the training set, because the composition of symmetry function values will change with the data set. In particular in early stages of the training set constructions, there might also be indi- vidual ACSFs with a very small range of values. Such symmetry functions should be preliminarily eliminated from the set of functions, because a narrow range of val- ues prevents proper scaling and can result in unstable training by learning numerical noise. Once the structural diversity in the data set increases resulting in an extended range of values, these ACSFs should be included again. For all these reasons, the set of ACSFs is not at all fixed in the training process but has to adapt to the increas- ing data set size for optimum performance, which can in principle be done in an automatic way. The final dimen- sionality of the ACSF vector is usually smaller than the formal dimensionality of the configuration space in the atomic cutoff spheres but strongly depends on the chem- ical composition of the system. Since the radial and an- gular symmetry functions contain all combinations of el- ements in the system, the number of descriptors strongly increases with the number of elements in the system.\n",
      "\n",
      "Box 3: Constructing a default set of ACSFs\n",
      "\n",
      "In early stages of HDNNP construction, often a\n",
      "\n",
      "11\n",
      "\n",
      "default set of ACSFs is used to provide a bal- anced description of all possible configurations in the atomic environments. Procedure for radial symmetry functions:\n",
      "\n",
      "For each element combination in the data set determine the minimum interatomic dis- tance Rmin.\n",
      "\n",
      "For each element combination set ηmax = to define the radial function with the 0 a−2 0 largest spatial extension.\n",
      "\n",
      "Determine for each element combination the value of ηmin such that the inflection point of the term gij = e−ηminR2 ij · fc(Rij) is located at Rmin. If this inflection point is close to Rc the cutoff should be increased.\n",
      "\n",
      "Select further η values between ηmax and ηmin for each element combination to obtain overall 5-6 radial functions with equidistant inflection points to yield a balanced radial resolution.\n",
      "\n",
      "Since by construction the minimum interatomic distance has been taken into account, all functions should have a reasonable range of values enabling the scaling of the ACSFs. Radial functions of element combinations, which are not present in the system, are left out.\n",
      "\n",
      "Procedure for angular symmetry functions\n",
      "\n",
      "Identify all possible element triplets.\n",
      "\n",
      "Set the exponent η = 0 a−2 0\n",
      "\n",
      "Set the exponent η = 0 a−2 0\n",
      "\n",
      "For each triplet generate angular functions with ζ = 1,2,4,16 for an approximately equidistant angular resolution.\n",
      "\n",
      "Combine each function with λ = 1,−1 to yield in total eight angular functions per el- ement triple.\n",
      "\n",
      "Check for each angular symmetry function the range of values and eliminate those func- tions with a too small range.\n",
      "\n",
      "Optionally, a second set of contracted angular symmetry functions can be constructed using a larger value of η to increase the angular sensitiv- ity close to the central atom.\n",
      "\n",
      "C.\n",
      "\n",
      "Initial Weight Parameters and Data\n",
      "\n",
      "Preparation\n",
      "\n",
      "Finally, while the number of weight parameters is de- fined by the architectures of the atomic NNs, a set of initial weight parameters has to be chosen as starting point for the HDNNP training. Apart from random num- bers, e.g. in the interval [−1,1], with normal or Gaussian distribution, several different methods for the selection of the starting weights have been proposed, such as the Nguyen Widrow scheme [122], in which weights are cho- sen such that each hidden node approximates a part of the target function, or the Xavier method [123], which aims to avoid values in the saturating region of the ac- tivation functions and is therefore especially suited for funnel-like architectures. However, the choice of the intial weights a priori does not ensure that the predicted energies and forces are close to the respective target values. This can be achieved by an additional preconditioning step before the training (see Fig. 7). If the initial weights are chosen randomly, the predicted mean energies and their standard deviations often differ significantly from the respective values of the reference data giving rise to a large error at the begin- ning of the training process. This error can be reduced by adjusting the initial weights such that the deviations between the mean energies of the HDNNP and the refer- ence data as well as the differences in their standard de- viations disappear. For this purpose, the mean HDNNP energy can be shifted by adjusting the bias weights of the output neuron, while the standard deviation can be con- trolled by additionally modifying the connecting weights between the neurons of the last hidden layer and the out- put layer.\n",
      "\n",
      "Further, some preprocessing is performed for the ref- erence data. Generally, splitting into a training and a test set is done (see Sec. VA), and also a third valida- tion set is often used. The difference between the test and the validation set is that the test set is used to iden- tify the potential with the minimum error for unknown structures. Since, however, the test set is thus involved in the selection of the potential, the latter is not com- pletely independent of the test set. For this reason, the validation set might provide a more unbiased estimate for the generalization properties of the potential. However, as discussed in Sec. VI, this splitting is not a reliable measure for the transferability of a potential since it is based on the assumption that the available reference set is covering the entire configuration space realiably, which cannot be taken for granted. Another preparation of the data set that is used if the range of output values is restricted by a hyperbolic tan- gent or sigmoid activation function in the output neuron is the scaling of the target energies to a unit interval, which in the application of the potential then has to be reversed. Moreover, depending on the choice of the ref- erence method, the target total energies can have very large values, which is numerically inconvenient. There-\n",
      "\n",
      "12\n",
      "\n",
      "EHDNNPErefσNNPσref\n",
      "\n",
      "E\n",
      "\n",
      "E\n",
      "\n",
      "Erefσref\n",
      "\n",
      "a)b)\n",
      "\n",
      "EHDNNPσNNP\n",
      "\n",
      "FIG. 7: Preconditioning of the neural network weights. In a first step, the mean energies ¯E and the standard deviations of the energies σ of the training data are determined for the HDNNP and the reference method (a). Then, the weight parameters are adjusted to match the respective values of the reference method to minimize the root mean squared error at the beginning of the training process (b).\n",
      "\n",
      "fore, often the energies of the free atoms of all elements in vacuum are removed from the data set such that instead numerically much smaller binding energies are used in the training. The resulting offset can then be corrected by adding the corresponding free atom energies when ap- plying the potential to yield total energies, which are numerically consistent with the reference method.\n",
      "\n",
      "Case study: Symmetry functions values, architecture and weights initialization of the LiOH-water system\n",
      "\n",
      "The symmetry function parameters of the LiOH- water system have been chosen following the recipe for generating a default set of ACSFs given in Box 3. As the values of the parameters η of the radial symmetry functions depend on the mini- mum distances between the atoms of the respec- tive element pairs in the reference data set, the set of symmetry function values has been adjusted throughout the construction of the HDNNP. In Table I the final η parameters of the converged potential are given, while the cutoff has been set\n",
      "\n",
      "to Rc = 6 ˚A and a shift of Rs = 0 ˚A has been used. The parameters of the angular symmetry functions have been selected as described in Box 3. In addition to the set of angular symmetry func- tions which has been derived following this proce- dure a second set of angular symmetry functions with the same values for ξ and λ but a value of η = 0.025 a−2 has been employed to better de- scribe the interaction of close atoms. Since there is only one lithium ion in the system and since the lithium ions in the periodic images are outside the cutoff radius, radial and angular ACSFs involving Li-Li interactions have been removed.\n",
      "\n",
      "0\n",
      "\n",
      "TABLE I: η values of the radial symmetry func- tions of the HDNNP for LiOH in water.\n",
      "\n",
      "Element pair\n",
      "\n",
      "η [a−2 0 ]\n",
      "\n",
      "H-H H-O H-Li O-O O-Li\n",
      "\n",
      "0.000, 0.006, 0.016, 0.038, 0.099 0.000, 0.007, 0.019, 0.051, 0.166 0.000, 0.005, 0.012, 0.025, 0.052 0.000, 0.004, 0.008, 0.015, 0.027 0.000, 0.005, 0.012, 0.024, 0.051\n",
      "\n",
      "For training the HDNNPs of the LiOH system the RuNNer code [32, 41] has been used. Various atomic NN architectures containing two or three hidden layers with 10 to 30 neurons per layer have been tested. The number of nodes was chosen to obtain a funnel-like architecture, i.e., each hidden layer contains less neurons than the previous hid- den layer. The highest accuracy was found em- ploying an architecture containing 25 nodes in the first hidden layer, 20 nodes in the second hidden layer, and 15 nodes in the third hidden layer re- sulting in a number of 3251 weights per element. The values of these weights were initialized follow- ing a modified Xavier initialization [124].\n",
      "\n",
      "V. TRAINING\n",
      "\n",
      "A. Optimization of the weight parameters\n",
      "\n",
      "The aim of the training process is to minimize the deviations between the HDNNP and the reference PES in the energetically relevant range. This is achieved by adjusting the values of the weight parameters using information from the reference data set. The optimiza- tion of the atomic NNs is a high-dimensional problem depending on thousands of weight and bias parameters and consequently it is impossible to find the global Still, there is a minimum in this parameter space. large number of roughly equivalent local minima of high quality, which represent the training data with low root mean squared errors (RMSE, see Box 4).\n",
      "\n",
      "13\n",
      "\n",
      "Box 4: Error calculation\n",
      "\n",
      "The quality of the potential is continuously mon- itored by computing the root mean squared error (RMSE) of the energies of all structures\n",
      "\n",
      "E RMSE =\n",
      "\n",
      "(cid:118) (cid:117) (cid:117) (cid:116)\n",
      "\n",
      "1 Nstruct\n",
      "\n",
      "Nstruct(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(Ei,Ref − Ei,HDNNP)2\n",
      "\n",
      "(8)\n",
      "\n",
      "and more rarely by the mean absolute deviation (MAD) or mean absolute error (MAE), which is typically smaller than the RMSE,\n",
      "\n",
      "E MAD =\n",
      "\n",
      "1 Nstruct\n",
      "\n",
      "Nstruct(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "|Ei,Ref − Ei,HDNNP|\n",
      "\n",
      ".\n",
      "\n",
      "(9)\n",
      "\n",
      "For the force RMSE, there are two possible defini- tions, which are both in use. The first refers to the force components of all atoms in all structures,\n",
      "\n",
      "F RMSE =\n",
      "\n",
      "(cid:118) (cid:117) (cid:117) (cid:116)\n",
      "\n",
      "1 Ncomp\n",
      "\n",
      "Ncomp (cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(Fi,Ref − Fi,HDNNP)2,\n",
      "\n",
      "(10)\n",
      "\n",
      "while the second is computed as the error in force vectors between the HDNNP and the reference method,\n",
      "\n",
      "F RMSE =\n",
      "\n",
      "(cid:118) (cid:117) (cid:117) (cid:116)\n",
      "\n",
      "1 Natoms\n",
      "\n",
      "Natoms(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(Fi,Ref − Fi,HDNNP)2,\n",
      "\n",
      "(11)\n",
      "\n",
      "for all atoms in the data set. The force RMSE with respect to the vectors is larger by a factor √ 3 compared to the component-based RMSE.\n",
      "\n",
      "The typical order of magnitude of the energy and force RMSEs of high-dimensional MLPs is about 1 meV/atom and 0.1 eV/˚A, respectively, but these values strongly de- pend on the diversity of atomic configurations, the qual- ity of the data set and the energy and force ranges to be represented. The energy error is usually normalized per atom to make the RMSE size-consistent. For instance, a primitive unit cell of an fcc metal containing only one atom should have the same energy error as the conven- tional cubic unit cell containing four atoms, since the structures are identical. The training is done iteratively presenting energies and forces of the training data again and again for a prede- fined number of epochs (iterations), i.e., by supervised learning. However, the training process contains much\n",
      "\n",
      "more than merely the optimization of the parameter val- ues. Instead, it also includes a continuous assessment of the quality of the potential (see Sec. VI), and the exten- sion of the data set by active learning (see Sec. VB). Typical data sets of about 10,000 structures, which may contain roughly 100 atoms each, provide a wealth of in- formation, i.e., 10,000 total energies and about 1,000,000 force vectors for each atom in the system, amounting in total to 3,010,000 pieces of information that can be exploited for the weight optimization. This is orders of magnitude larger than the number of parameters in HDNNPs, but this relation has to be treated with care as the formal amount of data does not contain any informa- tion about the diversity of the data set and its suitability to cover the configuration space of interest. For instance, the energy and force RMSEs will be low although the overall shape of the PES is not reliably represented if only a small subspace is sampled and all structures are very similar. Therefore, validating the PES quality for diverse structures, which are representative for the in- tended applications, is essential, and RMSE values alone can be difficult to interpret, because RMSEs can be arbi- trarily low when computed for homogeneous and strongly correlated data sets. The course of the training process is shown schemati- cally in Fig. 8. Starting with an initial set of weights (see Sec. IVC), in each epoch the optimization algorithm loops over all structures in the training set in random order. There are different ways to update the weights, batch-wise for a group of energies or forces, which may even comprise the entire data set, or after the presenta- tion of each individual piece of information in a point- by-point fashion corresponding to a batch-size of one. Since in a point-by-point training strategy the parame- ters are updated once for each energy and for each force component, an epoch contains a large number of weight adjustments resulting in a rapid progress of the training in terms of epochs, while numerous updates also increase the computational costs of each of these epochs. In par- ticular in case of large numbers of force components such a procedure can be very time-consuming. To speed-up the training process, several measures can be taken. First, in adaptive training algorithms [125] an error threshold is defined for the energies and forces, and only the energies and forces exceeding this threshold are used for the training, while those, which are already ac- curately represented, are skipped. A common procedure is to define the energy and force error thresholds in terms of the respective RMSEs of the previous epoch such that the update criteria become tighter along with the im- provement of the potential. As a consequence, in initial epochs when the RMSEs are still high only a few data points will be used and the overall shape of the PES will be learned quickly, while in later stages more information will be processed resulting in increasing computational costs of later epochs when the fine details of the PES are learned. Another optional way of reducing the number of weight\n",
      "\n",
      "14\n",
      "\n",
      "updates is to use batches of energy and forces and to perform an update only every Nth energy or force com- In this approach, the indi- ponent or even structure. vidual errors and their gradients are accumulated, fi- nally averaged, and used only for a single weight update per data group, which is particularly suited for paral- lelization. Such a parallelization is more difficult in case of constantly changing weight parameter values in the point-by-point approach in analogy to MD simulations, in which several MD steps cannot be parallelized due to the changing positions. Still, the latter update strategy is often more efficient, and reasonable potentials can be obtained after about 30-50 epochs, while in case of large batches that might even contain the entire training set thousands of epochs may be needed to reach a satisfac- tory accuracy of the potential. The most basic loss function ΓE to be minimized in the training for a set of Nbatch energies is given by\n",
      "\n",
      "ΓE =\n",
      "\n",
      "1 Nbatch\n",
      "\n",
      "Nbatch(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(Ei,Ref − Ei,HDNNP)2 .\n",
      "\n",
      "It should be noted that although the output values of the individual atomic NNs can be interpreted as atomic en- ergies, these are just mathematical auxiliary quantities without a physical meaning. Hence, ΓE is constructed using total binding energies only, without prior partition- ing into individual atomic energies. For gradient-based optimization algorithms the value and the derivative of this loss function with respect to the HDNNP parame- ters have to be computed. The analytic derivatives are efficiently available by recursion through backpropaga- tion [126]. Libraries for the construction of neural net- works such as TensorFlow [127] or PyTorch [128] directly provide access to these gradients. The loss function for the forces is defined in a similar way, but here the batch of Nbatch force components Fi over all atoms of all structures is evaluated,\n",
      "\n",
      "ΓF =\n",
      "\n",
      "β Nbatch\n",
      "\n",
      "Nbatch(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(Fi,Ref − Fi,HDNNP)2\n",
      "\n",
      ".\n",
      "\n",
      "The constant prefactor β can be used to balance the rel- ative impact of the energies and forces in a joint loss function ΓE + ΓF, but for instance in point-by-point up- date strategies such a joint loss function is not required. Once the loop over all structures as well as their ener- gies and forces has been completed and the respective updates of the weight parameters have been carried out, finally the new RMSE values of the energies and forces in the training as well as in the test set are computed and stored to assess the improvement of the potential. Then, the next epoch is started until the selected number of epochs has been completed and the final potential is ob- tained, which needs to be further validated (see Sec. VI). The weight updates are usually performed using such as the gradient-based optimization algorithms,\n",
      "\n",
      "(12)\n",
      "\n",
      "(13)\n",
      "\n",
      "EnergyUpdate\n",
      "\n",
      "TrainedPotentialCalculationoftheEnergyandForceRMSEsofallData\n",
      "\n",
      "CalculateEError\n",
      "\n",
      "CalculateGradient\n",
      "\n",
      "ForceUpdate\n",
      "\n",
      "LoopOverAllEpochsLoopOverAllStructures\n",
      "\n",
      "LoopOverAllForceComponentsGroupingGrouping\n",
      "\n",
      "CalculateGradient\n",
      "\n",
      "CalculateFError\n",
      "\n",
      "Initialization\n",
      "\n",
      "FIG. 8: Flowchart of the weight optimization process using energies and forces. In a point-by-point update strategy the weights are updated once per energy and per force component, while also batch-wise grouping of errors and gradients is possible. At the end of each epoch, the RMSEs of the entire training and also test sets are computed.\n",
      "\n",
      "global adaptive extended Kalman filter [125, 129, 130] (see Box 5), conjugate gradients [131], or the Levenberg- Marquardt algorithm [132]. Very simple algorithms, such as steepest descent are usually not used. Alternatively, for large data sets the computational per- formance can be improved using the method of stochastic gradient descent [133]. To optimize the convergence of this procedure momentum can be added or the learning rate can be modified, leading to algorithms such as the Adaptive Gradient Algorithm [134], AdaDelta [135] and the popular ADAM [136] optimizer.\n",
      "\n",
      "15\n",
      "\n",
      "An important decision for the accuracy of the potential that has been made in the training process is the archi- tecture of the atomic NNs, which governs the flexibility of the HDNNP. While too small networks prevent an accu- rate representation of the PES, too many parameters can give rise to overfitting, which can substantially reduce the transferability of the potential for structures not included in the training set. The presence of underfitting or over- fitting can be monitored by the early-stopping method. For this purpose, the available reference data set is split into a training set used for the optimization of the weight and bias parameters, and an independent test set includ- ing only structures unknown to the HDNNP. Then, the error of both data sets in the iterative training process is observed (see Fig. 9). If both, training and test set, do not reach low errors (Fig. 9a), the flexibility of the HDNNP is not large enough. If the flexibility of the HDNNP is increased, both errors decrease resulting in a reasonable fit quality for both, known and unknown structures (Fig. 9b). For too large atomic neural net- works, overfitting results in a close to perfect fit for the known training data, while artificial oscillations are in- troduced, which yield poor predictions for the test data (Fig. 9c). The reliability of the early-stopping method can be further improved by using cross-validation tech- niques and different splittings of the reference data, such that individual outliers are less relevant. Still, while use- ful for the determination of the required atomic NN ar- chitecture, the early-stopping method has to be used with great care as a tool to assess the overall quality of a po- tential as will be discussed in Sec. VI. Even with an optimal architecture overfitting cannot be completely avoided. Still, it can be further reduced by the use of regularization methods such as ridge regres- sion. As large values of the weights can be connected to overfitting, in ridge regression a term that penalizes large weight parameter values is added to the loss func- tion. Another possibility to avoid overfitting is to use dropout [137]. In this method nodes of the neural net- work are randomly dropped during training such that different thinned networks are trained. In this way com- plex co-adaptions are avoided in which layers would learn to correct mistakes of previous layers leading to bad gen- eralization properties of the network.\n",
      "\n",
      "Finally, several points have to be considered when restarting the training process from parameters obtained in a previous optimization. Since these parameters have been determined for a well-defined set of structures and their associated ACSF values, any scaling factors pos- sibly employed in the preprocessing of the ACSFs be- come part of the potential and need to be applied in the same way to newly added structures. If, for instance, the training set is increased before restarting the train- ing process, these scaling factors change. Consequently, the scaled symmetry function values are modified result- ing in slightly different energies and forces even for the structures included in the original data set and even if no adjustments in the weights have been made. This\n",
      "\n",
      "can be avoided by restarting the training process using fixed scaling factors at the cost of slight changes in the effective range of the ACSF values. Moreover, some opti- mization algorithms, such as the Kalman filter generate further information such as a covariance matrix in the fitting procedure, which must be stored are reused when restarting fits if a strict continuation of the training is intended (see Fig. 10).\n",
      "\n",
      "Box 5: Kalman Filter\n",
      "\n",
      "Originally, the Kalman filter has been developed to recursively find the minimum variance esti- mates of state variables of linear dynamic sys- tems, which has been generalized in the extended Kalman filter to non-linear systems [129]. Feed- forward neural networks can be considered as such non-linear dynamic systems and the Kalman filter can be efficiently used to find optimal weight pa- rameters while recursively looping through a set of reference data [125]. Generally, the update procedure is the same for all properties, which will be denoted here as V . According to the scheme in Fig. 8, the Kalman fil- ter is usually used for a batch size of one, i.e., a weight update is performed after each presented energy or force component, respectively. Each update k follows same procedure: First, the error is estimated as the absolute difference be- tween the predicted target value Vk and its refer- ence value V Ref and normalized by the number of atoms in the current structure Natoms\n",
      "\n",
      "k\n",
      "\n",
      "νk = |Vk − V ref\n",
      "\n",
      "k |N−1\n",
      "\n",
      "atoms .\n",
      "\n",
      "(14)\n",
      "\n",
      "In the following adaptive step this error is com- pared to an error threshold, which must be ex- ceeded to continue with the update process. Then, the Kalman gain matrix K is computed us- ing the Kalman covariance matrix of the previous step Pk−1, the current value of a forgetting fac- tor λ, the identity matrix I, and the Jabobian J containing the derivatives of the loss function with respect to all weight parameters,\n",
      "\n",
      "Kk = Pk−1Jk\n",
      "\n",
      "(cid:104)\n",
      "\n",
      "λk−1I + (Jk)T Pk−1Jk\n",
      "\n",
      "(cid:105)−1\n",
      "\n",
      ". (15)\n",
      "\n",
      "The covariance matrix is usually initialized as the unity matrix. During the training, it stores infor- mation about all updates. Its diagonal elements can be interpreted as the uncertainty of the cur- rent weight estimate. The gain matrix is then used to update the weight matrix w as\n",
      "\n",
      "wk = wk−1 + Kkνk .\n",
      "\n",
      "(16)\n",
      "\n",
      "In the following the covariance matrix and λ are\n",
      "\n",
      "16\n",
      "\n",
      "HDNNP Target\n",
      "\n",
      "EpochEpoch\n",
      "\n",
      "a)b)c)\n",
      "\n",
      "Energy (arb. units)Configuration\n",
      "\n",
      "Training point Test point\n",
      "\n",
      "Energy (arb. units)Configuration\n",
      "\n",
      "TrainingTest\n",
      "\n",
      "Energy (arb. units)Configuration\n",
      "\n",
      "E RMSE (arb. units) E RMSE (arb. units) E RMSE (arb. units)Epoch\n",
      "\n",
      "FIG. 9: Illustration of overfitting. The predictive power for structures of the target surface not included in the training can be estimated by comparing the root mean squared errors (RMSE) of the training set and an independent test set. Panel a) shows a typical HDNNP for the case of “underfitting” due to the use of small and thus inflexible neural networks. Both, the training and the test set RMSEs converge to high values (left) and neither the training nor the test points are well represented (right). The fine details of the potential energy surface cannot be reproduced and only rough features can be learned. Panel b) displays the RMSEs obtained when using more flexible neural networks, which can represent all details present in the training set. Also the test points are reliably predicted if they are not too different from the training data and if the flexibility is kept as low as necessary. The case of too flexible neural networks is shown in panel c). Here, the RMSE of the training set is very low, and as can be seen on the right, all training points are closely matched although the rapid oscillations resulting from the high flexibility prevent accurate predictions for the test data. This “overfitting”, which can be detected by comparing the RMSEs of the training and the test set (“early-stopping method”), can be reduced by also including gradient information, i.e., the forces, of the training points.\n",
      "\n",
      "updated\n",
      "\n",
      "Pk = λ−1 k\n",
      "\n",
      "I − Kk (Jk)T(cid:105) (cid:104)\n",
      "\n",
      "Pk−1 .\n",
      "\n",
      "λk = λk−1λ0 + 1 − λ0 .\n",
      "\n",
      "λ here takes the role of keeping the information\n",
      "\n",
      "(17)\n",
      "\n",
      "(18)\n",
      "\n",
      "of previous updates in the current update. Its ini- tial value λ0 is chosen close to unity and its value increases as the optimization progresses. As the value of λ increases, the amount of reference date points considered in each update also increases. This has the advantage to enable larger changes at the beginning of the optimizing process and thereby avoiding to get stuck in local minima. When the optimizing proceeds, more information\n",
      "\n",
      "17\n",
      "\n",
      "101\n",
      "\n",
      "10\n",
      "\n",
      "15\n",
      "\n",
      "log(Energy RMSE) [meV atom1]\n",
      "\n",
      "2\n",
      "\n",
      "20Epochs\n",
      "\n",
      "102\n",
      "\n",
      "0\n",
      "\n",
      "Weights restart\n",
      "\n",
      "8\n",
      "\n",
      "Weights and Kalman restart\n",
      "\n",
      "Continuous 20 epochs\n",
      "\n",
      "103\n",
      "\n",
      "18\n",
      "\n",
      "5\n",
      "\n",
      "12\n",
      "\n",
      "FIG. 10: Restarting the training process with the Kalman filter [125]. The violet curve shows a typical decrease of the energy RMSE in a training process consisting of 20 epochs. If however, the training is stopped after 10 epochs and shall be restarted, there are two options. If only the weight parameters of epoch 10 are used in the restart, the further progress of the training differs from the violet reference curve, since the covariance matrix of the Kalman filter is initialized again from scratch. If, however, this Kalman matrix is stored and used in the restart along with the weights, the obtained RMSE values are indistinguishable from the continuous fit of 20 epochs.\n",
      "\n",
      "is used and the local minima can be found by tak- ing smaller steps.\n",
      "\n",
      "B. Active Learning\n",
      "\n",
      "Since the quality of the potential obtained in the train- ing process critically depends on the available data set, an important task in the construction of MLPs is the gen- eration of reference data sets covering the relevant con- figuration space as comprehensively as possible. While in conventional empirical potentials and classical force fields a certain transferability is ensured by the physically inspired functional form, the extrapolation capabilities of MLPs are very limited and all information about the topology of the PES must be learned from the reference data. Therefore, the reference structures must cover the part of the PES, which is energetically accessible in the intended simulations. For the data set, not only condi- tions such as temperature and pressure, but depending on the intended application also different structures such as various polymorphs of crystal in case of solids or config- urations occurring in classical and quantum simulations of liquids such as water [92, 138] need to be considered. Further, repulsive structures for close atomic encounters need to be learned, which can also be assisted by explic-\n",
      "\n",
      "itly including two- and three-body terms [139, 140]. Since there is no hope to have at hand a complete set of all relevant structures before the training process, an automatic and unbiased approach to identify important atomic configurations is needed. Ideally, these structures should be determined taking the current training status of the MLP and the existing data into account. This approach is called active learning and is based on the concept of query by committee [141]. In this method, the high flexibility of MLPs, which is essential for their numerical accuracy but also the origin of failures for pre- dictions far from the training geometries, is turned into an advantage by making use of the energy or force vari- ance of predictions in an ensemble of MLPs when applied to unseen structures. To ensure that these structures are relevant for the intended applications of the MLP, they are often generated using similar simulation proto- cols and conditions including temperature and pressure as the final production simulations. If this variance is in the order of the RMSEs, then the prediction can be considered as reliable and the unseen structure is suffi- ciently close to a known training point. If, however, the variance is much larger, the unseen structure is far away from the training data and should be added to improve the potential in this region. In this way, step-by-step the data set and the quality of the MLP can be iteratively improved until a reliable potential has been generated. As a side note we mention that the use of a committee is not always necessary in all types of MLPs, as, for in- stance, in Gaussian process regression a single instance of the regressor can provide an estimate of the variance. The general procedure of active learning, which has be- come a standard in the development of modern MLPs [72, 73, 94, 142–145] is shown in Fig. 11. First, an initial data set is generated (see Sec. IIIC) and an ensemble of MLPs is trained as represented by two HDNNPs in Fig. 11a. All members of the ensemble should have ap- proximately the same quality in representing the avail- able training data, as measured, e.g., by the energy and force RMSEs. Then, one of these HDNNPs is chosen to generate a large number of trial structures, e.g. by the simulation method of interest, and the second HDNNP is used to recompute the energies and forces for these structures. Alternatively, an ensemble of HDNNPs can be used to predict these properties on-the-fly during the simulation. Then, the variance in the predictions is inves- tigated and structures with large deviations are selected for additional reference electronic structure calculations. These new data points are then added to the extended data set and used to train the next generation of MLPs. This procedure is repeated for several cycles until the variance of all trial structures remains close to the RM- SEs. The HDNNPs trained to this final data set are then ready for the production simulations. As can be seen in Fig. 11b, adding more and more data points reduces the gray-shaded regions of high uncertainty in the PES until a reliable representation of the PES has been obtained. Several comments should be made at this point. While in\n",
      "\n",
      "18\n",
      "\n",
      "H = E ψ ψ ^ Iteration 1 Iteration 2 Iteration 3 Iteration Na)b)\n",
      "\n",
      "...\n",
      "\n",
      "Inital data set\n",
      "\n",
      "Energy (arb. units)\n",
      "\n",
      "HDNNP 1HDNNP 2Target\n",
      "\n",
      "Additional reference calculationExtended data setTrain two or more HDNNPsFinal data set Select structureswith large deviation Recompute structures with HDNNP 2\n",
      "\n",
      "Convergence\n",
      "\n",
      "Energy (arb. units)\n",
      "\n",
      "HDNNP 1HDNNP 2\n",
      "\n",
      "Check prediction deviation\n",
      "\n",
      "Energy (arb. units)Energy (arb. units)Configuration\n",
      "\n",
      "Generate structures with HDNNP 1\n",
      "\n",
      "FIG. 11: Iterative improvement of HDNNPs by active learning. Panel a) shows a flowchart of the self-consistent determination of the required training data set. Starting from an initial data set, several HDNNPs are trained. One of these HDNNPs is then used to generate a large pool of structures. These structures are then recomputed by the remaining HDNNPs. If the variance of predicted properties, such as energies or forces in the ensemble are too large for some structures (framed in red) these are selected for additional reference calculations to increase the data set. This extended data set is then used for training in the next cycle and so forth until no poorly predicted structures are found and a converged potential has been obtained. In panel b) this procedure is shown schematically for an unknown target potential energy surface (violet line). The symbols represent the available training points. Close to these training points HDNNP 1 and HDNNP 2 predict very similar energies, while the regions with high uncertainty (grey) decrease with increasing data set until the final HDNNPs represent the potential energy surface in the entire region of interest.\n",
      "\n",
      "particular for Gaussian approximation potentials the re- training of MLPs on-the-fly during MD simulations has been suggested [146], this procedure is usually not ap- plied in case of HDNNPs. Instead, for neural network potentials often a few hundred new structures are first identified before a new generation of potentials is trained. Before this training step, new structures should be care- fully investigated since in particular in early stages of ac- tive learning unphysical structures may be suggested by poor preliminary potentials. Such structures complicate the training process while they are irrelevant for the final potential. Moreover, including similar structure should be avoided, which can be achieved, e.g., by farthest point sampling [147]. In general, monitoring the variance of the atomic forces should be preferred for the selection of new structures, since total energies are difficult to interpret in particular\n",
      "\n",
      "for large systems. The reason is the tendency for er- ror compensation among the individual atomic energies, which can lead to apparently reliable energy predictions but in fact reduce transferability. Moreover, the investi- gation of atomic forces allows to identify specific atomic environment, which are not well represented in the train- ing set. Transfering the respective atoms from complex systems to smaller structures for efficient reference cal- culations can be challenging, as a substantial part of the environment has to be included. For some systems the use of molecular fragments is a viable path to reduce the system size while keeping the relevant parts of the atomic environments intact [95, 97, 104, 105]. Active learning and farthest point sampling can also be used to select to minimum number of structures from an already existing data set needed to achieve a certain accuracy of the po- tential [145].\n",
      "\n",
      "19\n",
      "\n",
      "EMax\n",
      "\n",
      "a)b)\n",
      "\n",
      "EMin\n",
      "\n",
      "ERef [arb. units]EMinEMaxN [arb. units]\n",
      "\n",
      "N [arb. units]\n",
      "\n",
      "FIG. 12: Possible shapes of energy histograms of the reference data (schematic). Panel a) shows a histogram with balanced energy distribution in the energy range of interest. The lowest energy Emin is the energy of the optimized system in its global minimum configuration. Emax is the highest energy in the data set, which should be larger than the highest energy of interest. Panel b) shows an incomplete distribution of reference structures. Such isolated groups of structures can point to disconnected parts of trajectories in configuration space. These disconnected regions must be filled in the active learning process if transitions between these structure types are relevant for the intended simulations. The energies are normalized per atom to avoid offsets due to possible different chemical compositions.\n",
      "\n",
      "The convergence of the data set in the active learning process can also be investigated by plotting histograms of the energy distributions in the training set. Fig. 12a shows a balanced distribution of training structures start- ing from the smallest energy Emin, which is often the optimized global ground state structure of the system, and the highest energy of interest Emax. The possible presence of energy gaps, however, should be carefully in- vestigated (Fig.12b). Such gaps can arise from differ- ent chemical compositions of structures in the data set, which is in general no problem, but they can also point to missing parts of molecular trajectories. If, for example, a reaction from A to B shall be studied, a continuous representation of all intermediate structures and the re- spective energies in the data set is mandatory.\n",
      "\n",
      "Another common procedure to investigate the com- pleteness of the data set is the use of learning curves (see Fig. 13). Here, the RMSEs of the training set and the test set are plotted as a function of the training set size. Small training sets can be accurately learned while they are not representative for the entire configuration space. Therefore, the RMSE of an independent test set is very large. If the training set is increased, the learning task is more difficult and the training error increases while\n",
      "\n",
      "the overall quality of the MLP improves such that the RMSE of the test set decreases. For an ideal, infinitely large data set, training and test set RMSEs converge to very similar values. However, the usefulness of learning curves strongly depends on the coverage of configuration space. If certain structures are completely missing, they are absent in both, the training and the test set, and consequently not true convergence can be achieved.\n",
      "\n",
      "trainingsettestset\n",
      "\n",
      "ERMSEtrainingdata\n",
      "\n",
      "FIG. 13: Schematic learning curve. Small training sets can be learned with high accuracy, but the resulting HDNNP has poor generalization capabilities and provides high errors for test structures not included in the training set. If the number of training structures is increased, the training process is more challenging resulting in an increased training error, while the test set error decreases due to the overall improving potential. For very large training sets, the configuration space is well represented by the training data and the root mean squared errors (RMSE) of the training and test sets are very similar.\n",
      "\n",
      "Several variants of active learning have been proposed to reduce the computational effort of the reference cal- culations. In ∆-Learning [148–151] first a baseline po- tential is constructed. This baseline potential should be cost-effective to evaluate and can be, e.g., a simple em- pirical potential, a moderately expensive electronic struc- ture method, or another MLP trained to a large data set obtained from such a method. This baseline potential is then used to represent the rough overall topology of the PES. In a second step a MLP potential is trained to a represent the energy difference between the baseline potential and a very accurate high-level electronic struc- ture method. Since only a small energy range needs to be learned for this correction, its error is easier to con- trol. Moreover, the hope is that a smaller data set may be required compared to a conventional MLP completely based directly on high-level data. Still, as usually the corrugation of the PES is more or less independent of the choice of the reference method, the possibility for the reduction of the data set size is limited. An interesting use of ∆-Learning is the combination of a cost-effective\n",
      "\n",
      "20\n",
      "\n",
      "method providing energies and analytic forces, and an expensive method for which no forces are available [92]. In this case, at least the baseline potential benefits from the availability of additional force information. Finally, another approach is transfer learning [152]. Here, first a MLP is trained to a large and computationally af- fordable reference data set. A part of the training data is then recalculated at a higher level of theory and the potential is retrained to this data set. Some parameter of the MLP are fixed and do not change during the re- training while others can adjust to represent the modified shape of the PES at reduced costs.\n",
      "\n",
      "Case study: Training of the LiOH-water system\n",
      "\n",
      "The initial reference data set of the LiOH sys- tem was iteratively expanded following the ac- tive learning scheme. Six cycles of active learn- ing were conducted using the tool RuNNerActive- Learn [101]. In Fig. 14 histograms of the reference data set of each cycle are shown.\n",
      "\n",
      "157.45\n",
      "\n",
      "157.35\n",
      "\n",
      "200\n",
      "\n",
      "Cycle 01\n",
      "\n",
      "250\n",
      "\n",
      "350\n",
      "\n",
      "50\n",
      "\n",
      "AIMD\n",
      "\n",
      "157.25\n",
      "\n",
      "Cycle 02\n",
      "\n",
      "0\n",
      "\n",
      "157.40\n",
      "\n",
      "ERef [eVatoms1]\n",
      "\n",
      "Cycle 06\n",
      "\n",
      "400N\n",
      "\n",
      "Cycle 03\n",
      "\n",
      "Cycle 05\n",
      "\n",
      "300\n",
      "\n",
      "157.30\n",
      "\n",
      "100\n",
      "\n",
      "150\n",
      "\n",
      "157.20\n",
      "\n",
      "Cycle 05\n",
      "\n",
      "FIG. 14: Histograms of the lithium hydroxide data set at each cycle of the active learning process. The full data set also contains pure water struc- tures, which are not included in these histograms for clarity. The initial data set has been generated by ab initio MD (AIMD).\n",
      "\n",
      "The progress of the active learning can also be monitored by evaluating the RMSE values after each completed cycle of adding new structures and retraining the potential. The resulting RMSE val- ues are shown in Fig. 15. The obtained curve represents a learning curve, but there are notable differences in the shape of test set RMSE values compared to the idealized learning curve shown in Fig. 13, which would be found when extract- ing data from a close-to complete data set that is never available in any realistic scenario for com-\n",
      "\n",
      "plex systems. In the real case of Fig. 15, the data set increases from cycle to cycle and new parts of configuration space are explored, which thus in- crease the complexity in the training and the test set in the same way. Hence, the test points are described with comparable accuracy as the train- ing points illustrating the need for a careful use of learning curves in case of incomplete data sets. If at all, learning curves are best used for an al- ready large and converged data set starting from a small subset of data, which is then step-by-step in- creased, but they are not a very useful tool during active learning cycles.\n",
      "\n",
      "test\n",
      "\n",
      "1\n",
      "\n",
      "3E RMSE [meV atom1]\n",
      "\n",
      "training\n",
      "\n",
      "2\n",
      "\n",
      "training\n",
      "\n",
      "test\n",
      "\n",
      "7000\n",
      "\n",
      "4000\n",
      "\n",
      "8000N structures\n",
      "\n",
      "0.10F RMSE [eV a10]\n",
      "\n",
      "5000\n",
      "\n",
      "6000\n",
      "\n",
      "0.08\n",
      "\n",
      "FIG. 15: Change of the RMSE values during the active learning cycles. For consistency the same settings and architectures of HDNNPs have been used in all potentials. The radial symmetry func- tion η values are adjusted to the current data set.\n",
      "\n",
      "Overall, the most accurate HDNNP obtained for the final LiOH data set has energy RM- SEs of 1.204 meV atom−1 for the training and 1.682 meV atom−1 for the test data set. The force component RMSEs are 0.069272 eV a−1 for the 0 training and 0.069355 eV a−1 for the test data set.\n",
      "\n",
      "0\n",
      "\n",
      "C. Beyond Short-Ranged Potentials\n",
      "\n",
      "So far, we have discussed all the steps of the con- struction of MLPs for the example of second-generation HDNNPs. Most of these steps are general and equally apply to third- and fourth-generation HDNNPs. How- ever, the total energy expression of third- and fourth- generation HDNNPs consists of the electrostatic and the short-range parts (Eq. 7), which have to be trained se- quentially following the flowchart in Fig. 16. First, the energies, forces and atomic partial charges are determined in reference electronic structure calculations. Since atomic charges are not quantum mechanical ob-\n",
      "\n",
      "21\n",
      "\n",
      "ReferenceDataGeneration{}EirefEi,elecHDNNPEi,shortrefEirefEi,elecHDNNPFi,elecHDNNPFirefQirefChargeTrainingTrainingofShort-RangePartFinalHDNNPCalculationofandGenerationofShort-RangeTrainingSet,,=-Fi,shortrefFirefFi,elecHDNNP=-EiHDNNPEi,elecHDNNP=+FiHDNNPFi,elecHDNNPEi,shortHDNNPFi,shortHDNNP=+\n",
      "\n",
      "FIG. 16: Training of HDNNPs including long-range electrostatic interactions. First, the reference energies, forces and atomic charges are obtained from electronic structure calculations. Then, the charges are trained and the HDNNP electrostatic energies and forces are computed. After applying a screening (Eq. 19) these are then removed from the reference energies and forces to obtain the target energies and forces for the training of the short-range part. Once also the short-range part has been learned, the total HDNNP is given as a sum of the short range and electrostatic energies and forces (Eq. 7).\n",
      "\n",
      "servables, many different partitioning schemes can be used, such as Hirshfeld [153], Bader [154] or density- derived electrostatic and chemical (DDEC) charges [155]. Any numerical uncertainty resulting from this choice can be compensated inside the atomic environments by the short-range atomic energies. Then, the charges are learned, which is done either by using a second set of atomic NNs (third-generation HDNNPs) or by train- ing environment-dependent electronegativities (fourth- generation HDNNPs) yielding these charges in a charge equilibration process. Once an expression for the charges has been learned, the electrostatic energies and forces are computed and removed from the total reference energies\n",
      "\n",
      "and forces to determine the reference data for the short- range training. This sequential procedure has two rea- sons: first, a functional relation between the structure- dependent charges and the atomic positions needs to be available to compute the electrostatic forces, which contain the derivatives of the charges with respect to the atomic coordinates [38]. Such a relation cannot be obtained from the reference electronic structure calcu- lations. Second, since all those energy contributions, which are not electrostatic, are combined in the short- range part, there is no double counting of electrostatics by construction. Finally, the remaining short-range ener- gies and forces are trained to yield the HDNNP consisting of the electrostatic and the short-range part. The Coulomb potential has a singularity for short in- teratomic distances, which can give rise to an increased short-range energy range, which is more difficult to learn than the original reference energies. Therefore, the Coulomb interaction is usually screend to zero inside a screening radius Rscreen, which must be lower than the cutoff radius of the ACSFs as illustrated in Fig. 17. The screening function [91] (Fig. 17a) is given by\n",
      "\n",
      "fscreen =\n",
      "\n",
      "(cid:40)1 2 1\n",
      "\n",
      "(cid:104)\n",
      "\n",
      "1 − cos\n",
      "\n",
      "(cid:16) πRij Rscreen\n",
      "\n",
      "(cid:17)(cid:105)\n",
      "\n",
      "for Rij ≤ Rscreen for Rij > Rscreen.\n",
      "\n",
      "When multiplied by this screening function, the Coulomb potential decays smoothly to zero for small distances Rij (Fig. 17b). This screened Coulomb potential is then re- moved from the total energy reference curve before train- ing the short-range part (Fig. 17c). As can be seen for the example of an interatomic distance of about 1.2 ˚A, re- moving the screened Coulomb potential results in a much smaller energy range to be learned by the short-range part compared to the case of removing the unmodified Coulomb potential.\n",
      "\n",
      "VI. VALIDATION\n",
      "\n",
      "Due to the absence of physically restricted functional forms in MLPs, the validation is of central importance not only for the final potentials but also during the train- ing process and during the extension of the data set by active learning, as all these steps cannot be strictly sepa- rated. The validation is a multistep process, which is illustrated schematically in Fig.18. Starting from the available reference data set, a set of preliminary HDNNPs is constructed. For this purpose, the data set is split ran- domly into a training and a test set, and early-stopping is used to generate potentials with low test set RMSEs indi- cating good generalization of the potentials to structures not included in the training set. Like in the case of learn- ing curves discussed above, the early-stopping method can provide information about the density and complete- ness of the data in the configuration space that is covered by the reference data, but no assessment of the quality for other types of structures is possible.\n",
      "\n",
      "22\n",
      "\n",
      "(19)\n",
      "\n",
      "Coulomb with screening\n",
      "\n",
      "c)Rscreen\n",
      "\n",
      "4Eelec [arb.units]\n",
      "\n",
      "2\n",
      "\n",
      "1\n",
      "\n",
      "0.0\n",
      "\n",
      "2\n",
      "\n",
      "6\n",
      "\n",
      "1\n",
      "\n",
      "7\n",
      "\n",
      "0\n",
      "\n",
      "b)\n",
      "\n",
      "Potential - screened Coulomb\n",
      "\n",
      "1\n",
      "\n",
      "Reference potential\n",
      "\n",
      "1.0fscreen\n",
      "\n",
      "Coulomb no screening\n",
      "\n",
      "a)\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "0Eelec [arb.units]\n",
      "\n",
      "1\n",
      "\n",
      "8Rij [Å]\n",
      "\n",
      "4\n",
      "\n",
      "3\n",
      "\n",
      "0.5\n",
      "\n",
      "Potential - Coulomb\n",
      "\n",
      "FIG. 17: Screening of electrostatic interactions [91]. Panel a) shows the screening function fscreen (Eq. 19), which smoothly decays from one to zero inside the screening radius Rscreen = 6˚A. Panel b) shows the Coulomb energy of two atoms with opposite charges as well as the screened Coulomb interaction, which approaches zero for short interatomic distances. Inside the cutoff radius of the ACSFs the missing part of the Coulomb energy can be represented by the short-range atomic energies. Panel c) shows a reference pair potential Eref, as well as the energy curves obtained when removing the unscreened Coulomb energy (Eref − ECoulomb) or the screened Coulomb energy (Eref − Escreened). The curve obtained using the screened Coulomb potential covers a much smaller energy range and thus can be represented more accurately by the short-range energy.\n",
      "\n",
      "For all data in the training and the test sets, further anal- yses need to be done, since the RMSEs of the energies and forces provide only averaged information. Therefore, outliers and individual energies or forces, which are diffi- cult to learn, might remain undetected when inspecting RMSE values only. Fig. 19 shows the energy correlation plot relating the energies predicted by the HDNNP to the reference energies. For reasonable potentials all points should be located close to the line of perfect correlation. Such plots should be routinely investigated for the ener- gies and forces in the training as well as in the test set for all generated HDNNPs. Outliers in these plots can\n",
      "\n",
      "Checkforextrapolations\n",
      "\n",
      "Addstructurestoincreasedensityofdata\n",
      "\n",
      "e.g.:MolecularDynamicsSimulations\n",
      "\n",
      "Checkrobustnessoftheresults\n",
      "\n",
      "ReferencedatasetHDNNPtrainingtoobtainasetofpreliminaryHDNNPs\n",
      "\n",
      "ComparesimulationresultsobtainedwithdifferentHDNNPsofsimilarquality\n",
      "\n",
      "Extensionofcoveredconfigurationspace\n",
      "\n",
      "StructuregenerationusingHDNNPs\n",
      "\n",
      "Checkforholesinconfigurationspace\n",
      "\n",
      "FIG. 18: Multistep validation of machine learning potentials.\n",
      "\n",
      "often be related to problems in the underlying reference data, such as failed electronic convergence. A closer investigation of outliers is possible when plot- ting the correlation of errors obtained with two different potentials (see Fig.20). Points, which have a low error in one of the two or even in both potentials are acceptable, since the representation quality of individual points may differ from HDNNP to HDNNP. If, however, a point has large errors in both potentials, this is an indication of contradictory data in the training set. Such contradic- tions may arise, e.g, from inconsistent k-point sets (see Sec. IIIA) resulting in the assignment of different ener- gies or forces to very similar structural features. Such points cannot be learned and can be identified in error correlation plots for closer examination. It is important to note in this context that even a small number of prob- lematic data points can result in very poor HDNNPs, because they can have a strong impact on the weight op- timization thus affecting also the quality of the overall PES.\n",
      "\n",
      "In the next step, a large number of configurations is generated, ideally by the simulation technique that shall be used in the production calculations. These configu- rations should be searched for extrapolations, i.e., the existence of ACSF values outside the range of function values defined by the training set (see Fig. 21). This\n",
      "\n",
      "23\n",
      "\n",
      "a)b)\n",
      "\n",
      "FIG. 19: Energy correlation plots of the LiOH-water data set. Panel a) shows the signed energy errors and the density of data points as a function of the target energy of the reference method. Panel b) shows the correlation of the HDNNP energies and the energies of the reference method. To avoid energy offsets, binding energies are used for this purpose instead of total energies, which would strongly depend on the chemical composition. For a perfect potential, all points should be aligned along the diagonal line with slope of 45◦. Correlation plots should be generated separately for the training and the test data set for the energies as well as the force components.\n",
      "\n",
      "needs to be done separately for each ACSF, which is straightforward and computationally cost-effective, since information about the minimum and maximum values is typically available for each function from the prepara- tory scaling of the ACSFs before training. Since MLPs are not reliable in case of extrapolation, such situations must be avoided in the production simulations. This can be achieved by systematically searching for extrapolat- ing structures to extend the reference data with the aim to cover an increasing part of configuration space. It should be noted, however, that the absence of extrapo- lation is not a sufficient criterion for a reliable potential, as structural outliers can drastically extend the range of symmetry function values without a suitable coverage of configuration space. In such a situation, extrapolation in\n",
      "\n",
      "a)b)\n",
      "\n",
      "FIG. 20: Error correlation for different HDNNPs fitted on the LiOH data set. Panels a) and b) show the correlation between the training set errors of total energies and atomic forces with respect to the reference data obtained with two different HDNNPs trained to the same data set. Data points with high errors in both potentials indicate possible contradictory data, which cannot be learned due to insufficient the accuracy of the reference data. Such problems might arise from underconverged settings in the reference calculations (cf. Fig.5)) or incomplete electronic self-consistency.\n",
      "\n",
      "the descriptor space cannot be automatically detected. Another form of extrapolation refers to the potential en- ergy and forces in the system. Predictions of energies\n",
      "\n",
      "24\n",
      "\n",
      "or forces outside the range of values in the training set should be carefully checked in production simulations, as they might be less reliable. In principle, extrapolating structures can also be found in the active learning process, which is the most gen- eral approach, since deviations in the energy and force predictions for different HDNNPs are very likely in this case. However, reasonable predictions even in the case of extrapolation cannot be excluded, and in such situa- tions the elimination of extrapolating structures by active learning can be difficult. Once extrapolation occurs, sim- ulations should be stopped and moderately extrapolating structures should be included, while structures exhibit- ing very different descriptor values should be discarded since very unphysical structures occuring in later stages of extrapolating trajectories should not enter the refer- ence data set. As a simple check, for each newly added structure the interatomic distances should be computed and structures including too short bonds and other un- wanted structural features should be excluded. Much more difficult than the detection of extrapolation is the identification of regions in configuration space, which are not sufficiently sampled, i.e., “holes” in the multidi- mensional data set. Examples for points in such holes, which are formally not fulfilling the criterion of extrapo- lation are shown in Fig. 21. These points can be identi- fied by active learning as described above, which is thus a central component of potential validation. Only if the ac- tive learning process has been completed without finding further structures with substantial uncertainty, a reason- able transferability of the HDNNP can be expected. The last and most important quality check of a data set is the performance of the fitted HDNNP in applica- tions. Whenever possible, direct comparisons with data obtained directly with the reference electronic structure method should be made, including e.g. equilibrium ge- ometries and lattice parameters, vibrational frequencies and radial distribution functions. Ideally also indepen- dently trained HDNNPs, which have passed the full hi- erarchy of validation steps, should be employed to check on the robustness of the simulation results with respect to the specific parameterization.\n",
      "\n",
      "VII. CONCLUSIONS\n",
      "\n",
      "Much progress has been made in the past two decades in the construction of MLPs for atomistic simulations of large molecular and condensed systems. These de- velopments do not only concern the general methodical framework, such as the adaption and use of modern ma- chine learning algorithms and the derivation of suitable descriptors, but also the applicability to more and more complex systems. At the present time, the increasing availability of methods and their implementations in eas- ily accessible software packages has substantially lowered the barrier to the parameterization and use of MLPs. Still, this availability also bears some risks as the quality\n",
      "\n",
      "G1G1G2extrapolationinterpolationextrapolationextrapolationextrapolation\n",
      "\n",
      "minG2minG2maxG1max\n",
      "\n",
      "FIG. 21: Illustration of poorly represented regions in a two-dimensional configuration space spanned by coordinates G1 and G2. The blue points represent the available training data covering the grey-shaded region. Any structure outside the minimum and maximum values of G1 and G2 is formally extrapolating. The three green structures, however, are inside the intervals ,Gmax [Gmin 1 2 fulfill the criterion of extrapolation. Still, these structures are located in poorly represented regions and will be unreliable. Such structures can be identified by active learning.\n",
      "\n",
      ",Gmax 1\n",
      "\n",
      "] and [Gmin\n",
      "\n",
      "] and therefore do not\n",
      "\n",
      "2\n",
      "\n",
      "of MLPs is much more difficult to assess than the per- formance of conventional empirical potentials and force fields. While MLPs in principle can reach a very high accuracy, which is often indistinguishable from the underlying elec- tronic structure method, validation takes a central role as an excellent performance for some atomic configurations may go along with dramatic failures for other geome- tries. The use of “canned” potentials should therefore only be recommended if detailed information about the applicability of a potential for a specific system is avail- able. Unfortunately, no community standards have been established for such information yet. An important take-home message of this Tutorial is the insight that the parameterization, the generation of the reference data set and the validation of a potential are equally important steps, which cannot be separated from each other. An important example is active learning, in which preliminary, i.e., not yet perfect, potentials are used to identify structures missing in the training set. Hence, active learning also represents an important op- portunity for validation on-the-fly. An entire hierarchy\n",
      "\n",
      "25\n",
      "\n",
      "of validation steps is available, with active learning being the most general approach, which in principle also can cover the detection of overfitting and extrapolation. A lot has been achieved in (semi)automatic active learning schemes and the uncertainty quantification of MLPs dur- ing application, but the state of black-box methods has not yet been reached. Moreover, even some apparently simple tests, such as energy and force correlation plots, the analysis of emerging structures and the detailed in- spection of outliers, which can often be traced back to some problem in the data set, can contribute substan- tially to the improvement of potentials. Although this Tutorial takes the perspective of high-dimensional neu-\n",
      "\n",
      "[1] K. Burke and L. O. Wagner, Int. J. Quantum Chem.\n",
      "\n",
      "113, 96 (2012).\n",
      "\n",
      "[2] F. Nogueira, A. Castro, and M. A. L. Marques, “A tutorial on density functional theory,” in A Primer in Density Functional Theory (Springer Berlin Heidelberg, Berlin, Heidelberg, 2003) p. 218.\n",
      "\n",
      "[3] R. Car and M. Parrinello, Phys. Rev. Lett. 55, 2471\n",
      "\n",
      "(1985).\n",
      "\n",
      "[4] D. Marx and J. Hutter, Ab initio Molecular Dynamics: Basic Theory and Advanced Methods (Cambridge Uni- versity Press, 2009).\n",
      "\n",
      "[5] M. Born and R. Oppenheimer, Ann. Phys. 389, 457\n",
      "\n",
      "(1927).\n",
      "\n",
      "[6] J. Tersoff, Phys. Rev. Lett. 56, 632 (1986). [7] M. S. Daw and M. I. Baskes, Phys. Rev. B 29, 6443\n",
      "\n",
      "(1984).\n",
      "\n",
      "[8] N. L. Allinger, Y. H. Yuh, and J.-H. Lii, J. Am. Chem.\n",
      "\n",
      "Soc. 111, 8551 (1989).\n",
      "\n",
      "[9] W. D. Cornell, P. Cieplak, C. I. Bayly, I. R. Gould, Jr, K. M. Merz, D. M. Ferguson, D. C. Spellmeyer, T. Fox, J. W. Caldwell, and P. A. Kollman, J. Am. Chem. Soc. 117, 5179 (1995).\n",
      "\n",
      "[10] S. L. Mayo, B. D. Olafson, and W. A. Goddard, III, J.\n",
      "\n",
      "Phys. Chem. 94, 8897 (1990).\n",
      "\n",
      "[11] A. K. Rappe, C. J. Casewit, K. S. Colwell, W. A. God- dard, III, and W. M. Skiff, J. Am. Chem. Soc. 114, 10024 (1992).\n",
      "\n",
      "[12] B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D. J. States, S. Swaminathan, and M. Karplus, J. Comput. Chem. 4, 187 (1983).\n",
      "\n",
      "[13] A. C. Mater and M. L. Coote, J. Chem. Inf. Model. 59,\n",
      "\n",
      "2545 (2019).\n",
      "\n",
      "[14] H. M. Cartwritght, Machine Learning in Chemistry – The Impact of Artificial Intelligence (Royal Society of Chemistry, 2020).\n",
      "\n",
      "[15] A. Karthikeyan and U. D. Priyakumar, J. Chem. Sci.\n",
      "\n",
      "134, 2 (2021).\n",
      "\n",
      "[16] J. Gasteiger, A. Teckentrup, L. Terfloth, and S. Spy-\n",
      "\n",
      "cher, J. Phys. Org. Chem. 16, 232 (2003).\n",
      "\n",
      "[17] J. C. Gertrudes, V. G. Maltarollo, R. A. Silva, P. R. Oliveira, K. M. Honorio, and A. B. F. da Silva, Curr. Med. Chem. 19, 4289 (2012).\n",
      "\n",
      "[18] M. H. S. Segler, M. Preuss, and M. P. Waller, Nature\n",
      "\n",
      "555, 604 (2018).\n",
      "\n",
      "26\n",
      "\n",
      "ral network potentials, almost all discussed aspects are generally valid for a wide range of MLPs currently in use.\n",
      "\n",
      "ACKNOWLEDGMENTS\n",
      "\n",
      "This work was supported by the Deutsche Forschungs- gemeinschaft (DFG, German Research Founda- tion) under Germany’s Excellence Strategy—EXC 2033–390677874—RESOLV. Funding by the DFG (pri- ority program SPP 2363, project number 495842446) and discussions with Marco Eckhoff are gratefully acknowledged.\n",
      "\n",
      "[19] F. Strieth-Kalthff, F. Sandfort, M. H. S. Segler, and\n",
      "\n",
      "F. Glorius, Chem. Soc. Rev. 49, 6154 (2020).\n",
      "\n",
      "[20] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Fig- urnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. ˇZ´ıdek, A. Potapenko, A. Bridgland, C. Meyer, S. A. A. Kohl, A. J. Ballard, A. Cowie, B. Romera- Paredes, S. Nikolov, R. Jain, J. Adler, T. Back, S. Petersen, D. Reiman, E. Clancy, M. Zielinski, M. Steinegger, T. Berghammer, S. Bodenstein, D. Sil- ver, O. Vinyals, A. W. Senior, K. Kavukcuoglu, P. Kohli, and D. Hassabis, Nature 596, 583 (2021). [21] P. A. Unzueta, C. S. Greenwell, and G. J. O. Beran, J.\n",
      "\n",
      "Chem. Theory Comput. 17, 826 (2021).\n",
      "\n",
      "[22] M. S. Chen, T. J. Zuehlsdorff, T. Morawietz, C. M. Isborn, and T. E. Markland, J. Phys. Chem. Lett. 11, 7559 (2020).\n",
      "\n",
      "[23] T. B. Blank, S. D. Brown, A. W. Calhoun, and D. J.\n",
      "\n",
      "Doren, J. Chem. Phys. 103, 4129 (1995). [24] J. Behler, J. Chem. Phys. 145, 170901 (2016). [25] J. Behler and G. Cs´anyi, Eur. Phys. J. B 94, 142 (2021). [26] O. T. Unke, S. Chmiela, H. E. Sauceda, M. Gastegger, I. Poltavsky, K. T. Sch¨utt, A. Tkatchenko, and K.-R. M¨uller, Chem. Rev. 121, 10142 (2021).\n",
      "\n",
      "[27] P. Friederich, F. H¨ase, J. Proppe,\n",
      "\n",
      "and A. Aspuru-\n",
      "\n",
      "Guzik, Nat. Mater 20, 750 (2021).\n",
      "\n",
      "[28] F. No´e, A. Tkatchenko, K.-R. M¨uller, and C. Clementi,\n",
      "\n",
      "Ann. Rev. Phys. Chem. 71, 361 (2020).\n",
      "\n",
      "[29] C. M. Handley and J. Behler, Eur. Phys. J. B 87, 152\n",
      "\n",
      "(2014).\n",
      "\n",
      "[30] V. L. Deringer, M. A. Caro, and G. Cs´anyi, Adv. Mater.\n",
      "\n",
      "31, 1902765 (2019).\n",
      "\n",
      "[31] P. O. Dral, J. Phys. Chem. Lett. 11, 2336 (2020). [32] J. Behler, Int. J. Quantum Chem. 115, 1032 (2015). [33] E. Kocer, T. W. Ko, and J. Behler, Ann. Rev. Phys.\n",
      "\n",
      "Chem. 73, 163 (2022).\n",
      "\n",
      "[34] J. Behler, Phys. Chem. Chem. Phys. 13, 17930 (2011). [35] C. M. Handley and P. L. A. Popelier, J. Phys. Chem. A\n",
      "\n",
      "114, 3371 (2010).\n",
      "\n",
      "[36] T. W. Ko, J. A. Finkler, S. Goedecker, and J. Behler,\n",
      "\n",
      "Acc. Chem. Res. 54, 808 (2021).\n",
      "\n",
      "[37] S. Manzhos and T. Carrington, Jr, Chem. Rev. 121,\n",
      "\n",
      "10187 (2021).\n",
      "\n",
      "[38] J. Behler, Chem. Rev. 121, 10037 (2021).\n",
      "\n",
      "[39] J. Behler and M. Parrinello, Phys. Rev. Lett. 98, 146401\n",
      "\n",
      "(2007).\n",
      "\n",
      "[40] J. Behler, J. Phys.: Condens. Matter 26, 183001 (2014). [41] J. Behler, Angew. Chem. Int. Ed. 56, 12828 (2017). [42] J. Behler, J. Chem. Phys. 134, 074106 (2011). [43] J. S. Smith, O. Isayev, and A. E. Roitberg, Chem. Sci.\n",
      "\n",
      "8, 3192 (2017).\n",
      "\n",
      "[44] K. T. Sch¨utt, H. E. Sauceda, P.-J. Kindermans, A. Tkatchenko, and K.-R. M¨uller, J. Chem. Phys. 148, 241722 (2018).\n",
      "\n",
      "[45] S. Batzner, T. E. Smidt, L. Sun, J. P. Mailoa, M. Ko- rnbluth, N. Molinari, and B. Kozinsky, Nature Comm. 13, 2453 (2022).\n",
      "\n",
      "[46] L. Zhang, J. Han, H. Wang, R. Car, and W. E, Phys.\n",
      "\n",
      "Rev. Lett. 120, 143001 (2018).\n",
      "\n",
      "[47] A. P. Bart´ok, M. C. Payne, R. Kondor, and G. Cs´anyi,\n",
      "\n",
      "Phys. Rev. Lett. 104, 136403 (2010).\n",
      "\n",
      "[48] A. P. Bart´ok and G. Cs´anyi, Int. J. Quant. Chem. 115,\n",
      "\n",
      "1051 (2015).\n",
      "\n",
      "[49] A. V. Shapeev, Multiscale Model. Simul. 14, 1153\n",
      "\n",
      "(2016).\n",
      "\n",
      "[50] A. P. Thompson, L. P. Swiler, C. R. Trott, S. M. Foiles, and G. J. Tucker, J. Comp. Phys. 285, 316 (2015).\n",
      "\n",
      "[51] R. Drautz, Phys. Rev. B 99, 014104 (2019). [52] N. Artrith, T. Morawietz, and J. Behler, Phys. Rev. B\n",
      "\n",
      "83, 153101 (2011).\n",
      "\n",
      "[53] O. T. Unke and M. Meuwly, J. Chem. Theory Comput.\n",
      "\n",
      "15, 3678 (2019).\n",
      "\n",
      "[54] K. Yao, J. E. Herr, D. W. Toth, R. Mckintyre, and\n",
      "\n",
      "J. Parkhill, Chem. Sci. 9, 2261 (2018).\n",
      "\n",
      "[55] L. Zhang, H. Wang, M. C. Muniz, A. Z. Panagiotopou- los, R. Car, and W. E, J. Chem. Phys. 156, 124107 (2022).\n",
      "\n",
      "[56] B. C. B. Symons and P. L. A. Popelier, J. Chem. Theor.\n",
      "\n",
      "Comp. 18, 5577 (2022).\n",
      "\n",
      "[57] S. Houlding, S. Y. Liem, and P. L. A. Popelier, Int. J.\n",
      "\n",
      "Quantum Chem. 107, 2817 (2007).\n",
      "\n",
      "[58] T. Bereau, D. Andrienko, and O. A. von Lilienfeld, J.\n",
      "\n",
      "Chem. Theory Comput. 11, 3225 (2015).\n",
      "\n",
      "[59] T. Bereau, R. A. DiStasio, Jr, A. Tkatchenko, and O. A. von Lilienfeld, J. Chem. Phys. 148, 241706 (2018). [60] H. Muhli, X. Chen, A. P. Bart´ok, P. Hern´andez-Le´on, G. Cs´anyi, T. Ala-Nissila, and M. A. Caro, Phys. Rev. B 104, 054106 (2021).\n",
      "\n",
      "[61] N. T. P. Tu, N. Rezajooei, E. Johnson, and C. Rowley,\n",
      "\n",
      "Digital Discovery (2023), in press.\n",
      "\n",
      "[62] S. A. Ghasemi, A. Hofstetter, S. Saha, S. Goedecker, Phys. Rev. B 92, 045131 (2015).\n",
      "\n",
      "[63] X. Xie, K. A. Persson, and D. W. Small, J. Chem.\n",
      "\n",
      "Theory Comput. 16, 4256 (2020).\n",
      "\n",
      "[64] T. W. Ko, J. A. Finkler, S. Goedecker, and J. Behler,\n",
      "\n",
      "Nat. Commun. 12, 398 (2021).\n",
      "\n",
      "[65] D. P. Metcalf, A. Jiang, S. A. Spronk, D. L. Cheney, and C. D. Sherrill, J. Chem. Inf. Model. 61, 115 (2021). [66] L. Jacobson, J. Stevenson, F. Ramezanghorbani, and R. Abel,\n",
      "\n",
      "D. Ghoreishi, K. Leswing, E. Harder, J. Chem. Theor. Comp. 18, 2354 (2022).\n",
      "\n",
      "[67] A. K. Rappe and W. A. Goddard, III, J. Phys. Chem.\n",
      "\n",
      "95, 3358 (1991).\n",
      "\n",
      "[68] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl, in Proceedings of the 34th International Conference on Machine Learning, Proceedings of Ma- chine Learning Research, Vol. 70, edited by D. Precup and Y. W. Teh (PMLR, 2017) p. 1263.\n",
      "\n",
      "and\n",
      "\n",
      "[69] K. T. Sch¨utt, F. Arbabzadah, S. Chmiela, K. R. M¨uller, and A. Tkatchenko, Nature Comm. 8, 13890 (2017). [70] R. Zubatyuk, J. S. Smith, J. Leszczynski, and O. Isayev,\n",
      "\n",
      "Science Advances 5, eaav6490 (2019).\n",
      "\n",
      "[71] K. Sch¨utt, O. Unke,\n",
      "\n",
      "and M. Gastegger, in Proceed- ings of the 38th International Conference on Machine Learning, Proceedings of Machine Learning Research, Vol. 139, edited by M. Meila and T. Zhang (PMLR, 2021) pp. 9377–9388.\n",
      "\n",
      "[72] J. S. Smith, B. Nebgen, N. Lubbers, O. Isayev, and A. E. Roitberg, J. Chem. Phys. 148, 241733 (2018). [73] C. Schran, J. Behler, and D. Marx, J. Chem. Theory\n",
      "\n",
      "Comput. 16, 88 (2020).\n",
      "\n",
      "[74] C. Schran, F. L. Thiemann, P. Rowe, E. A. M¨uller, and A. Michaelides, PNAS 118,\n",
      "\n",
      "O. Marsalek, e2110077118 (2021).\n",
      "\n",
      "[75] J. B. Witkoskie and D. J. Doren, J. Chem. Theory Com-\n",
      "\n",
      "put. 1, 14 (2005).\n",
      "\n",
      "[76] J. D. Morrow, J. L. A. Gardner, and V. L. Deringer, J.\n",
      "\n",
      "Chem. Phys. 158, 121501 (2023).\n",
      "\n",
      "[77] A. P. Bart´ok, R. Kondor, and G. Cs´anyi, Phys. Rev. B\n",
      "\n",
      "87, 184115 (2013).\n",
      "\n",
      "[78] A. Sadeghi, S. A. Ghasemi, B. Schaefer, S. Mohr, M. A. Lill, and S. Goedecker, J. Chem. Phys. 139, 184118 (2013).\n",
      "\n",
      "[79] S. Jindal, S. Chiriki, and S. S. Bulusu, J. Chem. Phys.\n",
      "\n",
      "146, 204301 (2017).\n",
      "\n",
      "[80] A. S. Christensen, L. A. Bratholm, F. A. Faber, and O. A. von Lilienfeld, J. Chem. Phys. 152, 044107 (2020). [81] H. Huo and M. Rupp, Mach. Learn. Sci. Techn. 3,\n",
      "\n",
      "045017 (2022).\n",
      "\n",
      "[82] C. M. Bishop, Neural Networks for Pattern Recognition\n",
      "\n",
      "(Oxford University Press, 1996).\n",
      "\n",
      "[83] S. Haykin, Neural Networks and Learning Machines, 3rd\n",
      "\n",
      "ed. (Prentice Hall, 2008).\n",
      "\n",
      "[84] W. McCulloch and W. Pitts, Bull. Math. Biophys. 5,\n",
      "\n",
      "115 (1943).\n",
      "\n",
      "[85] J. Clark, Scientific Applications of Neural Nets (Springer, 2014-04-17) open Library ID: OL27997540M.\n",
      "\n",
      "[86] T. Kohonen, Neural Networks 1, 3 (1988). [87] K. Hornik, M. Stinchcombe, and H. White, Neural Net-\n",
      "\n",
      "works 2, 359 (1989).\n",
      "\n",
      "[88] S. N. Pozdnyakov, M. J. Willatt, A. P. Bart´ok, C. Ort- ner, G. Cs´anyi, and M. Ceriotti, Phys. Rev. Lett. 125, 166001 (2020).\n",
      "\n",
      "[89] S. Yue, M. C. Muniz, M. F. C. Andrade, L. Zhang, R. Car, and A. Z. Panagiotopoulos, J. Chem. Phys. 154, 034111 (2021).\n",
      "\n",
      "[90] P. P. Ewald, Ann. Phys. 64, 253 (1921). [91] T. Morawietz, V. Sharma,\n",
      "\n",
      "and J. Behler, J. Chem.\n",
      "\n",
      "Phys. 136, 064103 (2012).\n",
      "\n",
      "[92] J. Daru, H. Forbert, J. Behler, and D. Marx, Phys.\n",
      "\n",
      "Rev. Lett. 129, 226001 (2022).\n",
      "\n",
      "[93] M. S. Chen, J. Lee, H.-Z. Ye, T. C. Berkelbach, D. R. Reichman, and T. E. Markland, J. Chem. Theor. Comp. in press (2023).\n",
      "\n",
      "[94] N. Artrith and J. Behler, Phys. Rev. B 85, 045439\n",
      "\n",
      "(2012).\n",
      "\n",
      "[95] B. Huang and O. A. von Lilienfeld, Nat. Chem. 12, 945\n",
      "\n",
      "(2020).\n",
      "\n",
      "[96] H. E. Sauceda, S. Chmiela, I. Poltavsky, K.-R. M¨uller, and A. Tkatchenko, J. Chem. Phys. 150, 114102 (2019). [97] M. Eckhoff and J. Behler, J. Chem. Theory Comput.\n",
      "\n",
      "15, 3793 (2019).\n",
      "\n",
      "27\n",
      "\n",
      "[98] B. Delley, J. Comput. Chem. 17, 1152 (1996). [99] D. S. Cerutti and D. A. Case, J. Chem. Theory Comput.\n",
      "\n",
      "6, 443 (2010).\n",
      "\n",
      "[100] M. Eckhoff, K. N. Lausch, P. E. Bl¨ochl, and J. Behler,\n",
      "\n",
      "J. Chem. Phys. 153, 164107 (2020).\n",
      "\n",
      "[101] M. Eckhoff and J. Behler, npj Comp. Mater. 7, 170\n",
      "\n",
      "(2021).\n",
      "\n",
      "[102] R. Drautz, Phys. Rev. B 102, 024104 (2020). [103] M. R. G. Marques, J. Wolff, C. Steigemann,\n",
      "\n",
      "and M. A. L. Marques, Phys. Chem. Chem. Phys. 21, 6506 (2019).\n",
      "\n",
      "[104] M. Gastegger, C. Kauffmann, J. Behler, and P. Mar-\n",
      "\n",
      "quetand, J. Chem. Phys. 144, 194110 (2016).\n",
      "\n",
      "[105] M. Herbold and J. Behler, Phys. Chem. Chem. Phys.\n",
      "\n",
      "25, 12979 (2023).\n",
      "\n",
      "[106] A. Laio and M. Parrinello, Proc. Nat. Acad. Sci. USA\n",
      "\n",
      "99, 12562 (2002).\n",
      "\n",
      "[107] D. Shanavas Rasheeda, A. M. S. Daria, B. Schr¨oder, E. Matyus, and J. Behler, Phys. Chem. Chem. Phys. 24, 39281 (2022).\n",
      "\n",
      "[108] C. Draxl and M. Scheffler, MRS Bulletin 43, 676 (2018). [109] L. Talirz, S. Kumbhar, E. Passaro, A. V. Yakutovich, V. Granata, F. Gargiulo, M. Borelli, M. Uhrin, S. P. Huber, S. Zoupanos, C. S. Adorf, C. W. Andersen, O. Sch¨utt, C. A. Pignedoli, D. Passerone, J. Vande- Vondele, T. C. Schulthess, B. Smit, G. Pizzi, and N. Marzari, Scientific Data 7, 299 (2020).\n",
      "\n",
      "[110] CPPAW-Code, https://cppaw.org, accessed: 2023-05-\n",
      "\n",
      "31.\n",
      "\n",
      "[111] M. Sotoudeh, S. Rajpurohit, P. Bloechl, D. Mierwaldt, J. Norpoth, V. Roddatis, S. Mildner, B. Kressdorf, B. Ifland, and C. Jooss, Phys. Rev. B 95, 235150 (2017). [112] M. Eckhoff, P. E. Bl¨ochl, and J. Behler, Phys. Rev. B\n",
      "\n",
      "101, 205113 (2020).\n",
      "\n",
      "[113] P. E. Bl¨ochl, Phys. Rev. B 50, 17953 (1994). [114] S. Grimme, J. Antony, S. Ehrlich, and H. Krieg, J.\n",
      "\n",
      "Chem. Phys. 132, 154104 (2010).\n",
      "\n",
      "[115] M. Eckhoff and J. Behler, J. Chem. Phys. 155, 244703\n",
      "\n",
      "(2021).\n",
      "\n",
      "[116] V. L. Deringer and G. Cs´anyi, Phys. Rev. B 95, 094203\n",
      "\n",
      "(2017).\n",
      "\n",
      "[117] M. Herbold and J. Behler, J. Chem. Phys. 156, 114106\n",
      "\n",
      "(2022).\n",
      "\n",
      "[118] M. Gastegger, L. Schwiedrzik, M. Bittermann, F. Berzsenyi, and P. Marquetand, J. Chem. Phys. 148, 241709 (2018).\n",
      "\n",
      "[119] B. Casier, S. Carniato, T. Miteva, N. Capron, N. Sisourat, J. Chem. Phys 152, 234103 (2020). [120] G. Imbalzano, A. Anelli, D. Giofre, S. Klees, J. Behler,\n",
      "\n",
      "and M. Ceriotti, J. Chem. Phys. 148, 241730 (2018).\n",
      "\n",
      "[121] H. Gassner, M. Probst, A. Lauenstein, and K. Her-\n",
      "\n",
      "mansson, J. Phys. Chem. A 102, 4596 (1998).\n",
      "\n",
      "[122] D. Nguyen and B. Widrow, Int. Conf. Neur. Netw. 3,\n",
      "\n",
      "21 (1990).\n",
      "\n",
      "[123] X. Glorot and Y. Bengio, in Proceedings of the Thir- teenth International Conference on Artificial Intelli- gence and Statistics, Proceedings of Machine Learning Research, Vol. 9, edited by Y. W. Teh and M. Tittering- ton (PMLR, Chia Laguna Resort, Sardinia, Italy, 2010) pp. 249–256.\n",
      "\n",
      "[124] M. Eckhoff, F. Sch¨onewald, M. Risch, C. A. Volkert, P. E. Bl¨ochl, and J. Behler, Phys. Rev. B 102, 174102 (2020).\n",
      "\n",
      "and\n",
      "\n",
      "[125] T. B. Blank and S. D. Brown, J. Chemometrics 8, 391\n",
      "\n",
      "(1994).\n",
      "\n",
      "[126] D. E. Rumelhart, G. E. Hinton, and R. J. Williams,\n",
      "\n",
      "Nature 323, 533 (1986).\n",
      "\n",
      "[127] B. Pang, E. Nijkamp, and Y. N. Wu, J. Edu. Behav.\n",
      "\n",
      "Stat. 45, 227 (2020).\n",
      "\n",
      "[128] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Rai- son, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, in Advances in Neural Infor- mation Processing Systems 32 (Curran Associates, Inc., 2019) p. 8024.\n",
      "\n",
      "[129] R. E. Kalman, J. Basic Engineering 82, 35 (1960). [130] A. Singraber, T. Morawietz, J. Behler, and C. Dellago,\n",
      "\n",
      "J. Chem. Theory Comput. 15, 3075 (2019).\n",
      "\n",
      "[131] M. R. Hestenes and E. Stiefel, J. Res. Natl. Bur. Stand.\n",
      "\n",
      "49, 409 (1952).\n",
      "\n",
      "[132] K. Levenberg, Quart. Appl. Math. 2, 164 (1944). [133] S. Sra, S. Nowozin, and S. J. Wright, eds., Optimization for machine learning, Neural information processing se- ries (MIT Press, Cambridge, Mass, 2012).\n",
      "\n",
      "[134] J. Duchi, E. Hazan, and Y. Singer, Journal of Machine\n",
      "\n",
      "Learning Research 12, 2121 (2011).\n",
      "\n",
      "[135] M. D. Zeiler, “ADADELTA: An Adaptive Learning Rate\n",
      "\n",
      "Method,” (2012), arXiv:1212.5701 [cs].\n",
      "\n",
      "[136] D. P. Kingma and J. L. Ba, arXiv:1412.6980v9 (2018). [137] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, J. Mach. Learn. Res. 15, 1929 (2014).\n",
      "\n",
      "[138] V. Kapil, J. Behler, and M. Ceriotti, J. Chem. Phys.\n",
      "\n",
      "145, 234103 (2016).\n",
      "\n",
      "[139] P. Rowe, V. L. Deringer, P. Gasparotto, G. Cs´anyi, and\n",
      "\n",
      "A. Michaelides, J. Chem. Phys. 153, 034702 (2020).\n",
      "\n",
      "[140] T. W. Ko, J. A. Finkler, S. Goedecker, and J. Behler,\n",
      "\n",
      "arXiv:2305.10692v1 (2023).\n",
      "\n",
      "[141] H. S. Seung, M. Opper,\n",
      "\n",
      "and H. Sompolinsky, Pro- ceedings of the fifth annual workshop on computational learning theory , 287 (1992).\n",
      "\n",
      "[142] S. L. Frederiksen, K. W. Jacobsen, K. S. Brown, and J. P. Sethna, Phys. Rev. Lett. 93, 165501 (2004). [143] E. V. Podryabinkin and A. V. Shapeev, Comp. Mater.\n",
      "\n",
      "Sci. 140, 171 (2017).\n",
      "\n",
      "[144] L. Zhang, D.-Y. Lin, H. Wang, R. Car, and W. E, Phys.\n",
      "\n",
      "Rev. Mater. 3, 023804 (2019).\n",
      "\n",
      "[145] C. Schran, K. Brezina, and O. Marsalek, J. Chem. Phys.\n",
      "\n",
      "153, 104105 (2020).\n",
      "\n",
      "[146] R. Jinnouchi, F. Karsai, and G. Kresse, Phys. Rev. B\n",
      "\n",
      "100, 014105 (2019).\n",
      "\n",
      "[147] A. P. Bart´ok, S. De, C. Poelking, N. Bernstein, J. R. and M. Ceriotti, Sci. Adv. 3,\n",
      "\n",
      "Kermode, G. Cs´anyi, e1701816 (2017).\n",
      "\n",
      "[148] R. M. Balabin and E. I. Lomakina, J. Chem. Phys. 131,\n",
      "\n",
      "074104 (2009).\n",
      "\n",
      "[149] A. P. Bart´ok, M. J. Gillan, F. R. Manby, and G. Cs´anyi,\n",
      "\n",
      "Phys. Rev. B 88, 054104 (2013).\n",
      "\n",
      "[150] J. Wu and X. Xu, J. Chem. Phys. 127, 214105 (2007). [151] R. Ramakrishnan, P. O. Dral, M. Rupp, and O. A. von Lilienfeld, J. Chem. Theory Comput. 11, 2087 (2015). [152] J. S. Smith, B. T. Nebgen, R. Zubatyuk, N. Lubbers, C. Devereux, K. Barros, S. Tretiak, O. Isayev, and A. E. Roitberg, Nature Comm. 10, 2903 (2019).\n",
      "\n",
      "[153] F. L. Hirshfeld, Theor. Chim. Acta 44, 129 (1977). [154] R. F. W. Bader, Acc. Chem. Res. 18, 9 (1985).\n",
      "\n",
      "28\n",
      "\n",
      "[155] T. A. Manz and N. G. Limas, Roy. Soc. Adv. 6, 47771\n",
      "\n",
      "(2016).\n",
      "\n",
      "29\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a4511ba3aa682a52"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
